{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Oa6_ReJ1o3E3",
    "outputId": "207c0e10-030b-4e4d-d735-36d560c08dc6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import cv2\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "b7JCPTeGrp_h",
    "outputId": "fcfb974f-a0c1-4a5c-aad0-fa401f57821a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Exist...\n"
     ]
    }
   ],
   "source": [
    "my_link = './'\n",
    "DATA_DIR = my_link + 'dataset/'\n",
    "\n",
    "# load repo with data if it is not exists\n",
    "if not os.path.exists(DATA_DIR):\n",
    "  print('Dataset not exist...')\n",
    "else:\n",
    "  print('Dataset Exist...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "df8MD_gPru9W"
   },
   "source": [
    "### Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "glDCagxErrQF"
   },
   "outputs": [],
   "source": [
    "class DataGen(keras.utils.Sequence):\n",
    "    def __init__(self, ids, path, batch_size=8, image_size=(256,192), augmentation=None, preprocessing=None): # batch = 8 , image_size = 128x128\n",
    "        self.ids = ids\n",
    "        self.path = path\n",
    "        self.batch_size = batch_size\n",
    "        self.image_width = image_size[0]\n",
    "        self.image_height = image_size[1]\n",
    "        self.on_epoch_end()\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "        \n",
    "    def __load__(self, id_name):\n",
    "        ## get path\n",
    "        image_path = os.path.join(self.path, id_name, \"images\", id_name) + \".png\"\n",
    "        mask_path = os.path.join(self.path, id_name, \"masks/\")\n",
    "        all_masks = os.listdir(mask_path)\n",
    "        \n",
    "        ## reading Image\n",
    "        image = cv2.imread(image_path, 1)\n",
    "        b,g,r = cv2.split(image)\n",
    "        image = cv2.merge([r,g,b])\n",
    "        image = cv2.resize(image, (self.image_width, self.image_height))\n",
    "        \n",
    "        mask = np.zeros((self.image_height, self.image_width, 1))\n",
    "        \n",
    "        ## reading Masks\n",
    "        for name in all_masks:\n",
    "          if (not name=='desktop.ini'):\n",
    "            _mask_path = mask_path + name\n",
    "            _mask_image = cv2.imread(_mask_path, -1)\n",
    "            _mask_image = cv2.resize(_mask_image, (self.image_width, self.image_height))\n",
    "            _mask_image = np.expand_dims(_mask_image, axis=-1)\n",
    "            mask = np.maximum(mask, _mask_image)\n",
    "            \n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        else:\n",
    "            sample = get_validation_augmentation()(image=image, mask=mask)\n",
    "            _, mask = sample['image'], sample['mask']\n",
    "            del _\n",
    "\n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # minmax normalization \n",
    "        image = image/255.0\n",
    "        \n",
    "        return image, mask\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if(index+1)*self.batch_size > len(self.ids):\n",
    "            self.batch_size = len(self.ids) - index*self.batch_size\n",
    "        \n",
    "        files_batch = self.ids[index*self.batch_size : (index+1)*self.batch_size]\n",
    "        \n",
    "        image = []\n",
    "        mask  = []\n",
    "        \n",
    "        for id_name in files_batch:\n",
    "            _img, _mask = self.__load__(id_name)  \n",
    "            image.append(_img)\n",
    "            mask.append(_mask)\n",
    "            \n",
    "        image = np.array(image)\n",
    "        mask  = np.array(mask)\n",
    "\n",
    "        return image, mask\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.ids)/float(self.batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y0RoPupDr0pH"
   },
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "\n",
    "def round_clip_0_1(x, **kwargs):\n",
    "    return x.round().clip(0, 1)\n",
    "\n",
    "# define training augmentations\n",
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "\n",
    "        A.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n",
    "\n",
    "        A.PadIfNeeded(min_height=image_height, min_width=image_width, always_apply=True, border_mode=0),\n",
    "        A.RandomCrop(height=image_height, width=image_width, always_apply=True),\n",
    "\n",
    "        A.IAAPerspective(p=0.5),\n",
    "\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.CLAHE(p=1),\n",
    "                A.RandomBrightness(p=1),\n",
    "                A.RandomGamma(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.IAASharpen(p=1),\n",
    "                A.Blur(blur_limit=3, p=1),\n",
    "                A.MotionBlur(blur_limit=3, p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.RandomContrast(p=1),\n",
    "                A.HueSaturationValue(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "        A.Lambda(mask=round_clip_0_1)\n",
    "    ]\n",
    "    return A.Compose(train_transform)\n",
    "\n",
    "# define testing augmentations\n",
    "def get_validation_augmentation():\n",
    "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "    test_transform = [\n",
    "        A.Lambda(mask=round_clip_0_1)\n",
    "    ]\n",
    "    return A.Compose(test_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "ASQnlwESr27d",
    "outputId": "caf8084b-7dcc-46b1-c2a5-d1ec0eace8ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_17_ids: 2000\n",
      "valid_17_ids: 150\n",
      "test_17_ids: 600\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# load txt file for ids of image in train validation test split\n",
    "\n",
    "train_path = DATA_DIR\n",
    "\n",
    "## Training Ids\n",
    "train_ids = next(os.walk(train_path))[1]\n",
    "\n",
    "load_new_ids = {}\n",
    "new_dataset_path = os.listdir(my_link+'dataset/')\n",
    "for data_path in new_dataset_path:\n",
    "    with open(my_link+'dataset/'+data_path, 'rb') as handle: \n",
    "        load_new_ids[data_path] = [str(line.rstrip())[2:-1] for line in handle]\n",
    "\n",
    "train_17_ids = load_new_ids['ISIC2017_Training.txt']\n",
    "test_17_ids = load_new_ids['ISIC2017_Test.txt']\n",
    "valid_17_ids = load_new_ids['ISIC2017_Validation.txt']\n",
    "\n",
    "print(\"train_17_ids:\",len(train_17_ids))\n",
    "print(\"valid_17_ids:\",len(valid_17_ids))\n",
    "print(\"test_17_ids:\",len(test_17_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aZHDPFGor5CS"
   },
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "lr = 1e-3\n",
    "epochs = 200\n",
    "batch_size = 8\n",
    "image_width = 256\n",
    "image_height = 192"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "few8NQONsDLk"
   },
   "source": [
    "### Dataset Preparation For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dWy5o7CjsBfR"
   },
   "outputs": [],
   "source": [
    "def set_data(train_ids,test_ids,valid_ids):\n",
    "    train_gen = DataGen(train_ids, train_path, image_size=(image_width, image_height), batch_size=batch_size, augmentation=get_training_augmentation())\n",
    "    test_gen = DataGen(test_ids, train_path, image_size=(image_width, image_height), batch_size=1, augmentation=get_validation_augmentation())\n",
    "    valid_gen = DataGen(valid_ids, train_path, image_size=(image_width, image_height), batch_size=batch_size, augmentation=get_validation_augmentation())\n",
    "\n",
    "    train_steps = len(train_ids)//batch_size\n",
    "    test_steps = len(test_ids)\n",
    "    valid_steps = len(valid_ids)//batch_size\n",
    "\n",
    "    return train_gen,test_gen,valid_gen,train_steps,test_steps,valid_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rZQErVU_sHYK"
   },
   "outputs": [],
   "source": [
    "# get data generator and step variable\n",
    "train_gen,test_gen,valid_gen,train_steps,test_steps,valid_steps = set_data(train_17_ids,test_17_ids,valid_17_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "g003XkkbsJCs",
    "outputId": "2a2045ed-f914-440e-ddff-2dab6f1a8b2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 192, 256, 3) (8, 192, 256, 1)\n",
      "[0. 1.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# random plotting\n",
    "x, y = train_gen.__getitem__(0)\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "r = random.randint(0, len(x)-1)\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.imshow(x[r])\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.imshow(np.reshape(y[r], (image_height, image_width)), cmap=\"gray\")\n",
    "print(np.unique(y[r]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gpJ63d-dtuaY"
   },
   "source": [
    "###Import SM Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "x_DH2gkTsLPI",
    "outputId": "c6ba6845-bc01-4137-dff1-35ff85eca164"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `keras` framework.\n"
     ]
    }
   ],
   "source": [
    "import segmentation_models as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bGXEs8u0t6AJ"
   },
   "source": [
    "####Block Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UOa6j4Gw3A_b"
   },
   "outputs": [],
   "source": [
    "from keras.applications import keras_applications\n",
    "from keras_applications.mobilenet_v3 import MobileNetV3Large\n",
    "\n",
    "# UNET BLOCK\n",
    "def unet_decoder_block(input, filters, skip, stage):\n",
    "\n",
    "  x = keras.layers.UpSampling2D((2, 2), name='decoder_stage{}_upsampling'.format(stage))(input)\n",
    "\n",
    "  if skip is not None:\n",
    "    x = keras.layers.Concatenate(name='decoder_stage{}_concat'.format(stage))([x, skip])\n",
    "\n",
    "  x = keras.layers.Conv2D(filters, kernel_size=(3,3), padding='same', strides=1, use_bias=False, name='decoder_stage{}a_conv'.format(stage))(x)\n",
    "  x = keras.layers.BatchNormalization(axis=3, name='decoder_stage{}a_bn'.format(stage))(x)\n",
    "  x = keras.layers.Activation('relu', name='decoder_stage{}a_relu'.format(stage))(x)\n",
    "\n",
    "  x = keras.layers.Conv2D(filters, kernel_size=(3,3), padding='same', strides=1, use_bias=False, name='decoder_stage{}b_conv'.format(stage))(x)\n",
    "  x = keras.layers.BatchNormalization(axis=3, name='decoder_stage{}b_bn'.format(stage))(x)\n",
    "  x = keras.layers.Activation('relu', name='decoder_stage{}b_relu'.format(stage))(x)\n",
    "\n",
    "  return x\n",
    "\n",
    "# SEPARABLE BLOCK\n",
    "def convolution_block(x, filters, stage, size, strides=(1,1), padding='same', activation=True):\n",
    "    x = keras.layers.SeparableConv2D(filters, size, strides=strides, padding=padding, name='decoder_stage{}_sepconv'.format(stage))(x)\n",
    "    x = keras.layers.BatchNormalization(name='decoder_stage{}_bn'.format(stage))(x)\n",
    "    if activation == True:\n",
    "        x = keras.layers.LeakyReLU(alpha=0.1, name='decoder_stage{}_leakyrelu'.format(stage))(x)\n",
    "    return x\n",
    "\n",
    "def residual_block(blockInput, num_filters=16, stage=''):\n",
    "    x = keras.layers.LeakyReLU(alpha=0.1)(blockInput)\n",
    "    x = keras.layers.BatchNormalization(name='decoder_stage{}_bn'.format(stage))(x)\n",
    "    blockInput = keras.layers.BatchNormalization(name='decoder_stage{}_res_bn'.format(stage))(blockInput)\n",
    "    x = convolution_block(x, num_filters, str(stage)+'0', (3,3) )\n",
    "    x = convolution_block(x, num_filters, str(stage)+'1', (3,3), activation=False)\n",
    "    x = keras.layers.Add(name='decoder_stage{}_add'.format(stage))([x, blockInput])\n",
    "    return x\n",
    "\n",
    "def unet_decoder_block_separable(input, filters, skip, stage):\n",
    "\n",
    "  x = keras.layers.Conv2DTranspose(filters, (3, 3), strides=(2, 2), padding=\"same\")(input)\n",
    "  print(x.shape)\n",
    "\n",
    "  if skip is not None:\n",
    "    x = keras.layers.Concatenate(name='decoder_stage{}_concat'.format(stage))([x, skip])\n",
    "    x = keras.layers.Dropout(0.1)(x)\n",
    "\n",
    "  x = keras.layers.SeparableConv2D(filters, (3, 3), activation=None, padding=\"same\", name='decoder_stage{}_sepconv'.format(stage))(x)\n",
    "  x = residual_block(x,filters,str(stage)+'a')\n",
    "  x = residual_block(x,filters,str(stage)+'b')\n",
    "  x = keras.layers.LeakyReLU(alpha=0.1, name='decoder_stage{}_leakyrelu'.format(stage))(x)\n",
    "\n",
    "  return x\n",
    "\n",
    "# LSTM BLOCK \n",
    "def unet_decoder_block_lstm(input, filters, skip, stage):\n",
    "  if skip is None:\n",
    "    x = keras.layers.UpSampling2D((2, 2), name='decoder_stage{}_upsampling'.format(stage))(input)\n",
    "  else:\n",
    "    x = keras.layers.Conv2DTranspose(int(skip.shape[-1]), kernel_size=2, strides=2, padding='same', use_bias=False, kernel_initializer=\"he_normal\", name='decoder_stage{}_up_transpose_conv'.format(stage))(input)\n",
    "    x = keras.layers.BatchNormalization(axis=3, name='decoder_stage{}_up_bn'.format(stage))(x)\n",
    "    x = keras.layers.Activation('relu', name='decoder_stage{}_up_relu'.format(stage))(x) \n",
    "  \n",
    "  if skip is not None:\n",
    "    x1 = keras.layers.Reshape(target_shape=(1, int(skip.shape[-3]), int(skip.shape[-2]), int(skip.shape[-1])), name='decoder_stage{}_reshape1'.format(stage))(x)\n",
    "    x2 = keras.layers.Reshape(target_shape=(1, int(skip.shape[-3]), int(skip.shape[-2]), int(skip.shape[-1])), name='decoder_stage{}_reshape2'.format(stage))(skip)\n",
    "    x  = keras.layers.concatenate([x1,x2], axis = 1, name='decoder_stage{}_concat'.format(stage)) \n",
    "    x = keras.layers.ConvLSTM2D(filters = filters, kernel_size=(3, 3), padding='same', return_sequences = False, go_backwards = True, kernel_initializer=\"he_normal\", name='decoder_stage{}_conv2d_lstm'.format(stage))(x)\n",
    "\n",
    "  x = keras.layers.Conv2D(filters, kernel_size=(3,3), padding='same', strides=1, use_bias=False, name='decoder_stage{}a_conv'.format(stage))(x)\n",
    "  x = keras.layers.BatchNormalization(axis=3, name='decoder_stage{}a_bn'.format(stage))(x)\n",
    "  x = keras.layers.Activation('relu', name='decoder_stage{}a_relu'.format(stage))(x)\n",
    "\n",
    "  x = keras.layers.Conv2D(filters, kernel_size=(3,3), padding='same', strides=1, use_bias=False, name='decoder_stage{}b_conv'.format(stage))(x)\n",
    "  x = keras.layers.BatchNormalization(axis=3, name='decoder_stage{}b_bn'.format(stage))(x)\n",
    "  x = keras.layers.Activation('relu', name='decoder_stage{}b_relu'.format(stage))(x)\n",
    "\n",
    "  return x\n",
    "\n",
    "# BCDU BLOCK\n",
    "def unet_decoder_block_bcdu(input, filters, skip, stage):\n",
    "  if skip is not None: \n",
    "    x = keras.layers.Conv2DTranspose(int(skip.shape[-1]), kernel_size=2, strides=2, padding='same', use_bias=False, kernel_initializer=\"he_normal\", name='decoder_stage{}_up_transpose_conv'.format(stage))(input)\n",
    "  else:\n",
    "    x = keras.layers.Conv2DTranspose(filters*2, kernel_size=2, strides=2, padding='same', use_bias=False, kernel_initializer=\"he_normal\", name='decoder_stage{}_up_transpose_conv'.format(stage))(input)\n",
    "\n",
    "  x = keras.layers.BatchNormalization(axis=3, name='decoder_stage{}_up_bn'.format(stage))(x)\n",
    "  x = keras.layers.Activation('relu', name='decoder_stage{}_up_relu'.format(stage))(x) \n",
    "  \n",
    "  if skip is not None:\n",
    "    x1 = keras.layers.Reshape(target_shape=(1, int(skip.shape[-3]), int(skip.shape[-2]), int(skip.shape[-1])), name='decoder_stage{}_reshape1'.format(stage))(x)\n",
    "    x2 = keras.layers.Reshape(target_shape=(1, int(skip.shape[-3]), int(skip.shape[-2]), int(skip.shape[-1])), name='decoder_stage{}_reshape2'.format(stage))(skip)\n",
    "    x  = keras.layers.concatenate([x1,x2], axis = 1, name='decoder_stage{}_concat'.format(stage)) \n",
    "    x = keras.layers.ConvLSTM2D(filters = filters, kernel_size=(3, 3), padding='same', return_sequences = False, go_backwards = True, kernel_initializer=\"he_normal\", name='decoder_stage{}_conv2d_lstm'.format(stage))(x)\n",
    "\n",
    "  x = keras.layers.Conv2D(filters*2, kernel_size=(3,3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', strides=1, use_bias=False, name='decoder_stage{}a_conv'.format(stage))(x)\n",
    "  x = keras.layers.Conv2D(filters*2, kernel_size=(3,3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', strides=1, use_bias=False, name='decoder_stage{}b_conv'.format(stage))(x)\n",
    "\n",
    "  if skip is None:\n",
    "    x = keras.layers.Conv2D(2, kernel_size=(3,3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', strides=1, use_bias=False, name='decoder_stage{}c_conv'.format(stage))(x)\n",
    "\n",
    "  return x\n",
    "\n",
    "\n",
    "def MobileNetV3_Encoder(decoder=None, input_shape=(256, 256)):\n",
    "\n",
    "  inputs = keras.layers.Input((input_shape[0], input_shape[1], 3))\n",
    "  \n",
    "  # GET MOBILENETV3 BACKBONE\n",
    "  backbone = MobileNetV3Large(input_tensor=inputs, input_shape=(input_shape[0], input_shape[1], 3), include_top=False, weights='imagenet', backend=keras.backend, layers=keras.layers, models=keras.models, utils=keras.utils)\n",
    "\n",
    "  # RENAME LAYER TO SOLVE THE PROBLEM WHEN SAVING THE MODEL\n",
    "  for i, layer in enumerate(backbone.layers[1:]):\n",
    "    layer.name = 'layer_' + str(i) + '_' + layer.name\n",
    "\n",
    "  # GET SKIP CONNECTION\n",
    "  skip1 = backbone.get_layer(index=13).output\n",
    "  skip2 = backbone.get_layer(index=31).output\n",
    "  skip3 = backbone.get_layer(index=79).output\n",
    "  skip4 = backbone.get_layer(index=146).output\n",
    "\n",
    "  if decoder is None:\n",
    "    return \"Please choose your decoder block!\"\n",
    "\n",
    "  if decoder.lower() == 'unet':\n",
    "    # UNET DECODER BLOCK\n",
    "    x = unet_decoder_block(backbone.output, 256, skip4, 0)\n",
    "    x = unet_decoder_block(x, 128, skip3, 1)\n",
    "    x = unet_decoder_block(x, 64, skip2, 2)\n",
    "    x = unet_decoder_block(x, 32, skip1, 3)\n",
    "    x = unet_decoder_block(x, 16, None, 4)\n",
    "  \n",
    "  if decoder.lower() == 'lstm':\n",
    "    # lstm DECODER BLOCK\n",
    "    x = unet_decoder_block_lstm(backbone.output, 256, skip4, 0)\n",
    "    x = unet_decoder_block_lstm(x, 128, skip3, 1)\n",
    "    x = unet_decoder_block_lstm(x, 64, skip2, 2)\n",
    "    x = unet_decoder_block_lstm(x, 32, skip1, 3)\n",
    "    x = unet_decoder_block_lstm(x, 16, None, 4)\n",
    "\n",
    "  if decoder.lower() == 'bcdu':\n",
    "    # bcdu DECODER BLOCK\n",
    "    x = unet_decoder_block_bcdu(backbone.output, 256, skip4, 0)\n",
    "    x = unet_decoder_block_bcdu(x, 128, skip3, 1)\n",
    "    x = unet_decoder_block_bcdu(x, 64, skip2, 2)\n",
    "    x = unet_decoder_block_bcdu(x, 32, skip1, 3)\n",
    "    x = unet_decoder_block_bcdu(x, 16, None, 4)\n",
    "\n",
    "  if decoder.lower() == 'separable':\n",
    "    # separable DECODER BLOCK\n",
    "    x = unet_decoder_block_separable(backbone.output, 256, skip4, 0)\n",
    "    x = unet_decoder_block_separable(x, 128, skip3, 1)\n",
    "    x = unet_decoder_block_separable(x, 64, skip2, 2)\n",
    "    x = unet_decoder_block_separable(x, 32, skip1, 3)\n",
    "    x = unet_decoder_block_separable(x, 16, None, 4)\n",
    "\n",
    "  x = keras.layers.Conv2D(1, kernel_size=(3,3), padding='same', strides=1, kernel_initializer=\"he_normal\")(x)\n",
    "\n",
    "  outputs = keras.layers.Activation(\"sigmoid\")(x)\n",
    "\n",
    "  model = keras.models.Model(inputs=inputs, outputs=outputs, name = 'MobileNetV3_{}'.format(decoder.title()))\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S-0GbNn3uYFF"
   },
   "source": [
    "####Preparing Model for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "colab_type": "code",
    "id": "Mxk1qrnzuNmn",
    "outputId": "6ecf3f11-473a-48fc-8bf7-c148098f1e8d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras_applications\\mobilenet_v3.py:348: UserWarning: `input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  warnings.warn('`input_shape` is undefined or non-square, '\n"
     ]
    }
   ],
   "source": [
    "#########------------------------#########\n",
    "#                                        #\n",
    "#         MASUKKAN MODEL DISINI!         #\n",
    "#                                        #\n",
    "#########------------------------#########\n",
    "\n",
    "\n",
    "model = MobileNetV3_Encoder(decoder='unet', input_shape=(192, 256))\n",
    "\n",
    "# model = sm.Unet('resnet50', input_shape=(192, 256, 3), encoder_weights='imagenet', classes=1, activation='sigmoid')\n",
    "# model = sm.Unet('vgg16', input_shape=(192, 256, 3), encoder_weights='imagenet', classes=1, activation='sigmoid')\n",
    "\n",
    "models_to_compile = [\n",
    "                     model\n",
    "                    ]\n",
    "models_to_name = [\n",
    "                  'MobileNetV3_UNet_aug_swaConst20_jacc_{}_{}_{}x{}_{}'.format(lr,epochs,image_height,image_width,'yes'),\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FAgBVyvUuPm7"
   },
   "outputs": [],
   "source": [
    "for model_, name_ in zip(models_to_compile, models_to_name):\n",
    "    model_.name = name_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aViZpxlmupVJ"
   },
   "outputs": [],
   "source": [
    "# define optomizer\n",
    "optim = keras.optimizers.Adam(lr)\n",
    "\n",
    "# Segmentation losses function can be combined together by '+' and scaled by integer or float factor\n",
    "jaccard_loss = sm.losses.JaccardLoss(per_image=True)\n",
    "dice_loss = sm.losses.DiceLoss(per_image=True)\n",
    "bce_loss = sm.losses.BinaryCELoss()\n",
    "\n",
    "total_loss = jaccard_loss\n",
    "\n",
    "metrics = [sm.metrics.IOUScore(per_image=True), sm.metrics.FScore(per_image=True), sm.metrics.Precision(per_image=True), sm.metrics.Recall(per_image=True)]\n",
    "\n",
    "# compile keras model with defined optimozer, loss and metrics\n",
    "for model_ in models_to_compile:\n",
    "    model_.compile(optim, total_loss, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wj4mBJKfuysN"
   },
   "source": [
    "#### Training Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qDPbWfNUusSU"
   },
   "outputs": [],
   "source": [
    "save_dir =  [\n",
    "              my_link+'models/',\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "riLYGxFnu60W"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from keras.utils import plot_model\n",
    "import json, codecs, pickle\n",
    "\n",
    "from swa.tfkeras import SWA\n",
    "\n",
    "# helper function for training visualization\n",
    "def train_vis(history, model_save_dir, doing=\"show\"):\n",
    "    # Plot training & validation iou_score values\n",
    "    plt.figure(figsize=(30, 5))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(history['iou_score'])\n",
    "    plt.plot(history['val_iou_score'])\n",
    "    plt.title('Model iou_score')\n",
    "    plt.ylabel('iou_score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.subplot(122)\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "    if (doing==\"save\"):\n",
    "        plt.savefig(model_save_dir+'_plot.png')\n",
    "        print(\"Success Saving Plot\")\n",
    "        plt.clf()\n",
    "    else: \n",
    "        plt.show()\n",
    "\n",
    "# helper function for training \n",
    "def train_fit(model_, epochs=epochs, pretrain=False):\n",
    "    print(\"Training for \"+model_.name)\n",
    "    try:\n",
    "        os.mkdir(save_dir[0]+model_.name)\n",
    "    except FileExistsError:\n",
    "        print('Directory not created, '+model_.name+' was exist!')\n",
    "\n",
    "    # define swa callback\n",
    "    swa = SWA(start_epoch=epochs-int(epochs*0.1), \n",
    "              lr_schedule='constant', \n",
    "              swa_lr=0.0001,\n",
    "              verbose=1)\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(save_dir[0]+\"{}/{}\".format(model_.name,model_.name)+'_best_weights.h5', monitor='val_iou_score', save_weights_only=True, save_best_only=True, mode='max'),\n",
    "        swa,\n",
    "    ]\n",
    "    \n",
    "    model_.fit_generator( train_gen, \n",
    "                          validation_data=valid_gen, \n",
    "                          steps_per_epoch=train_steps, \n",
    "                          validation_steps=valid_steps, \n",
    "                          epochs=epochs,\n",
    "                          callbacks=callbacks)\n",
    "    \n",
    "    model_.save_weights(save_dir[0]+model_.name+\"/{}_last_weight.h5\".format(model_.name))\n",
    "    \n",
    "    if pretrain:\n",
    "        return 0\n",
    "\n",
    "    model_save_dir = save_dir[0]+model_.name+'/'+model_.name\n",
    "\n",
    "    train_vis(model_.history.history, model_save_dir, doing=\"save\")\n",
    "\n",
    "    with open(model_save_dir+'_history.bin', 'wb') as handle:\n",
    "        pickle.dump(model_.history.history, handle)\n",
    "    \n",
    "    model_.save(model_save_dir+'.h5')\n",
    "    print(\"Success Saving Model\")\n",
    "\n",
    "# helper function for testing \n",
    "def test_eval(model_, text):\n",
    "    test_gen = DataGen(test_17_ids, train_path, image_size=(image_width, image_height), batch_size=1, augmentation=get_validation_augmentation())\n",
    "    i_=0\n",
    "    list_of_test = []\n",
    "    print(\"Testing for \"+model_.name)\n",
    "    scores = model_.evaluate_generator(test_gen)\n",
    "    list_of_test.append(\"Loss: {:.5}\".format(scores[0]))\n",
    "    print(list_of_test[i_])\n",
    "    for metric, value in zip(metrics, scores[1:]):\n",
    "        i_ += 1\n",
    "        list_of_test.append(\"mean {}: {:.5}\".format(metric.__name__, value))\n",
    "        print(list_of_test[i_])\n",
    "\n",
    "    model_save_dir = save_dir[0]+model_.name+'/'+model_.name\n",
    "    \n",
    "    with open(model_save_dir+'_'+text+'_scores.txt', 'w') as f:\n",
    "        for item in list_of_test:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    \n",
    "\n",
    "# helper function for data visualization\n",
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "    \n",
    "# helper function for data visualization    \n",
    "def denormalize(x):\n",
    "    \"\"\"Scale image to range 0..1 for correct plot\"\"\"\n",
    "    x_max = np.percentile(x, 98)\n",
    "    x_min = np.percentile(x, 2)    \n",
    "    x = (x - x_min) / (x_max - x_min)\n",
    "    x = x.clip(0, 1)\n",
    "    return x\n",
    "\n",
    "def test_vis(model_):\n",
    "    test_ids = test_17_ids_new ### Set to Dataset \n",
    "\n",
    "    # new definition test data \n",
    "    test_dataset = DataGen(test_ids, train_path, image_size=(image_width, image_height), batch_size=1)\n",
    "    \n",
    "    # visualize\n",
    "    n = 5\n",
    "    ids = np.random.choice(np.arange(len(test_ids)), size=n)\n",
    "\n",
    "    for i in ids:\n",
    "        \n",
    "        image, gt_mask = test_dataset[i]\n",
    "        image = np.expand_dims(image[0], axis=0)\n",
    "        pr_mask = model_.predict(image).round()\n",
    "        \n",
    "        visualize(\n",
    "            image=denormalize(image.squeeze()),\n",
    "            gt_mask=gt_mask[0][..., 0].squeeze(),\n",
    "            pr_mask=pr_mask[0][..., 0].squeeze(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SFUKwz_xvBFT"
   },
   "source": [
    "#### Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Q7c7X2NP5YgO",
    "outputId": "6e270716-fe9e-40c9-df6b-d5e40730c16e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"MobileNetV3_UNet_aug_swaConst20_jacc_0.001_200_192x256_yes\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 192, 256, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "layer_0_Conv_pad (ZeroPadding2D (None, 193, 257, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_1_Conv (Conv2D)           (None, 96, 128, 16)  432         layer_0_Conv_pad[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_2_Conv/BatchNorm (BatchNo (None, 96, 128, 16)  64          layer_1_Conv[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3_activation_1 (Activatio (None, 96, 128, 16)  0           layer_2_Conv/BatchNorm[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_4_expanded_conv/depthwise (None, 96, 128, 16)  144         layer_3_activation_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_5_expanded_conv/depthwise (None, 96, 128, 16)  64          layer_4_expanded_conv/depthwise[0\n",
      "__________________________________________________________________________________________________\n",
      "layer_6_activation_3 (Activatio (None, 96, 128, 16)  0           layer_5_expanded_conv/depthwise/B\n",
      "__________________________________________________________________________________________________\n",
      "layer_7_expanded_conv/project ( (None, 96, 128, 16)  256         layer_6_activation_3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_8_expanded_conv/project/B (None, 96, 128, 16)  64          layer_7_expanded_conv/project[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_9_expanded_conv/Add (Add) (None, 96, 128, 16)  0           layer_3_activation_1[0][0]       \n",
      "                                                                 layer_8_expanded_conv/project/Bat\n",
      "__________________________________________________________________________________________________\n",
      "layer_10_expanded_conv_1/expand (None, 96, 128, 64)  1024        layer_9_expanded_conv/Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "layer_11_expanded_conv_1/expand (None, 96, 128, 64)  256         layer_10_expanded_conv_1/expand[0\n",
      "__________________________________________________________________________________________________\n",
      "layer_12_activation_4 (Activati (None, 96, 128, 64)  0           layer_11_expanded_conv_1/expand/B\n",
      "__________________________________________________________________________________________________\n",
      "layer_13_expanded_conv_1/depthw (None, 97, 129, 64)  0           layer_12_activation_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_14_expanded_conv_1/depthw (None, 48, 64, 64)   576         layer_13_expanded_conv_1/depthwis\n",
      "__________________________________________________________________________________________________\n",
      "layer_15_expanded_conv_1/depthw (None, 48, 64, 64)   256         layer_14_expanded_conv_1/depthwis\n",
      "__________________________________________________________________________________________________\n",
      "layer_16_activation_5 (Activati (None, 48, 64, 64)   0           layer_15_expanded_conv_1/depthwis\n",
      "__________________________________________________________________________________________________\n",
      "layer_17_expanded_conv_1/projec (None, 48, 64, 24)   1536        layer_16_activation_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_18_expanded_conv_1/projec (None, 48, 64, 24)   96          layer_17_expanded_conv_1/project[\n",
      "__________________________________________________________________________________________________\n",
      "layer_19_expanded_conv_2/expand (None, 48, 64, 72)   1728        layer_18_expanded_conv_1/project/\n",
      "__________________________________________________________________________________________________\n",
      "layer_20_expanded_conv_2/expand (None, 48, 64, 72)   288         layer_19_expanded_conv_2/expand[0\n",
      "__________________________________________________________________________________________________\n",
      "layer_21_activation_6 (Activati (None, 48, 64, 72)   0           layer_20_expanded_conv_2/expand/B\n",
      "__________________________________________________________________________________________________\n",
      "layer_22_expanded_conv_2/depthw (None, 48, 64, 72)   648         layer_21_activation_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_23_expanded_conv_2/depthw (None, 48, 64, 72)   288         layer_22_expanded_conv_2/depthwis\n",
      "__________________________________________________________________________________________________\n",
      "layer_24_activation_7 (Activati (None, 48, 64, 72)   0           layer_23_expanded_conv_2/depthwis\n",
      "__________________________________________________________________________________________________\n",
      "layer_25_expanded_conv_2/projec (None, 48, 64, 24)   1728        layer_24_activation_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_26_expanded_conv_2/projec (None, 48, 64, 24)   96          layer_25_expanded_conv_2/project[\n",
      "__________________________________________________________________________________________________\n",
      "layer_27_expanded_conv_2/Add (A (None, 48, 64, 24)   0           layer_18_expanded_conv_1/project/\n",
      "                                                                 layer_26_expanded_conv_2/project/\n",
      "__________________________________________________________________________________________________\n",
      "layer_28_expanded_conv_3/expand (None, 48, 64, 72)   1728        layer_27_expanded_conv_2/Add[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_29_expanded_conv_3/expand (None, 48, 64, 72)   288         layer_28_expanded_conv_3/expand[0\n",
      "__________________________________________________________________________________________________\n",
      "layer_30_activation_8 (Activati (None, 48, 64, 72)   0           layer_29_expanded_conv_3/expand/B\n",
      "__________________________________________________________________________________________________\n",
      "layer_31_expanded_conv_3/depthw (None, 51, 67, 72)   0           layer_30_activation_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_32_expanded_conv_3/depthw (None, 24, 32, 72)   1800        layer_31_expanded_conv_3/depthwis\n",
      "__________________________________________________________________________________________________\n",
      "layer_33_expanded_conv_3/depthw (None, 24, 32, 72)   288         layer_32_expanded_conv_3/depthwis\n",
      "__________________________________________________________________________________________________\n",
      "layer_34_activation_9 (Activati (None, 24, 32, 72)   0           layer_33_expanded_conv_3/depthwis\n",
      "__________________________________________________________________________________________________\n",
      "layer_35_expanded_conv_3/squeez (None, 72)           0           layer_34_activation_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_36_reshape_1 (Reshape)    (None, 1, 1, 72)     0           layer_35_expanded_conv_3/squeeze_\n",
      "__________________________________________________________________________________________________\n",
      "layer_37_expanded_conv_3/squeez (None, 1, 1, 24)     1752        layer_36_reshape_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_38_expanded_conv_3/squeez (None, 1, 1, 24)     0           layer_37_expanded_conv_3/squeeze_\n",
      "__________________________________________________________________________________________________\n",
      "layer_39_expanded_conv_3/squeez (None, 1, 1, 72)     1800        layer_38_expanded_conv_3/squeeze_\n",
      "__________________________________________________________________________________________________\n",
      "layer_40_activation_10 (Activat (None, 1, 1, 72)     0           layer_39_expanded_conv_3/squeeze_\n",
      "__________________________________________________________________________________________________\n",
      "layer_41_expanded_conv_3/squeez (None, 24, 32, 72)   0           layer_34_activation_9[0][0]      \n",
      "                                                                 layer_40_activation_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_42_expanded_conv_3/projec (None, 24, 32, 40)   2880        layer_41_expanded_conv_3/squeeze_\n",
      "__________________________________________________________________________________________________\n",
      "layer_43_expanded_conv_3/projec (None, 24, 32, 40)   160         layer_42_expanded_conv_3/project[\n",
      "__________________________________________________________________________________________________\n",
      "layer_44_expanded_conv_4/expand (None, 24, 32, 120)  4800        layer_43_expanded_conv_3/project/\n",
      "__________________________________________________________________________________________________\n",
      "layer_45_expanded_conv_4/expand (None, 24, 32, 120)  480         layer_44_expanded_conv_4/expand[0\n",
      "__________________________________________________________________________________________________\n",
      "layer_46_activation_11 (Activat (None, 24, 32, 120)  0           layer_45_expanded_conv_4/expand/B\n",
      "__________________________________________________________________________________________________\n",
      "layer_47_expanded_conv_4/depthw (None, 24, 32, 120)  3000        layer_46_activation_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_48_expanded_conv_4/depthw (None, 24, 32, 120)  480         layer_47_expanded_conv_4/depthwis\n",
      "__________________________________________________________________________________________________\n",
      "layer_49_activation_12 (Activat (None, 24, 32, 120)  0           layer_48_expanded_conv_4/depthwis\n",
      "__________________________________________________________________________________________________\n",
      "layer_50_expanded_conv_4/squeez (None, 120)          0           layer_49_activation_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_51_reshape_2 (Reshape)    (None, 1, 1, 120)    0           layer_50_expanded_conv_4/squeeze_\n",
      "__________________________________________________________________________________________________\n",
      "layer_52_expanded_conv_4/squeez (None, 1, 1, 32)     3872        layer_51_reshape_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_53_expanded_conv_4/squeez (None, 1, 1, 32)     0           layer_52_expanded_conv_4/squeeze_\n",
      "__________________________________________________________________________________________________\n",
      "layer_54_expanded_conv_4/squeez (None, 1, 1, 120)    3960        layer_53_expanded_conv_4/squeeze_\n",
      "__________________________________________________________________________________________________\n",
      "layer_55_activation_13 (Activat (None, 1, 1, 120)    0           layer_54_expanded_conv_4/squeeze_\n",
      "__________________________________________________________________________________________________\n",
      "layer_56_expanded_conv_4/squeez (None, 24, 32, 120)  0           layer_49_activation_12[0][0]     \n",
      "                                                                 layer_55_activation_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_57_expanded_conv_4/projec (None, 24, 32, 40)   4800        layer_56_expanded_conv_4/squeeze_\n",
      "__________________________________________________________________________________________________\n",
      "layer_58_expanded_conv_4/projec (None, 24, 32, 40)   160         layer_57_expanded_conv_4/project[\n",
      "__________________________________________________________________________________________________\n",
      "layer_59_expanded_conv_4/Add (A (None, 24, 32, 40)   0           layer_43_expanded_conv_3/project/\n",
      "                                                                 layer_58_expanded_conv_4/project/\n",
      "__________________________________________________________________________________________________\n",
      "layer_60_expanded_conv_5/expand (None, 24, 32, 120)  4800        layer_59_expanded_conv_4/Add[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_61_expanded_conv_5/expand (None, 24, 32, 120)  480         layer_60_expanded_conv_5/expand[0\n",
      "__________________________________________________________________________________________________\n",
      "layer_62_activation_14 (Activat (None, 24, 32, 120)  0           layer_61_expanded_conv_5/expand/B\n",
      "__________________________________________________________________________________________________\n",
      "layer_63_expanded_conv_5/depthw (None, 24, 32, 120)  3000        layer_62_activation_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_64_expanded_conv_5/depthw (None, 24, 32, 120)  480         layer_63_expanded_conv_5/depthwis\n",
      "__________________________________________________________________________________________________\n",
      "layer_65_activation_15 (Activat (None, 24, 32, 120)  0           layer_64_expanded_conv_5/depthwis\n",
      "__________________________________________________________________________________________________\n",
      "layer_66_expanded_conv_5/squeez (None, 120)          0           layer_65_activation_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_67_reshape_3 (Reshape)    (None, 1, 1, 120)    0           layer_66_expanded_conv_5/squeeze_\n",
      "__________________________________________________________________________________________________\n",
      "layer_68_expanded_conv_5/squeez (None, 1, 1, 32)     3872        layer_67_reshape_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_69_expanded_conv_5/squeez (None, 1, 1, 32)     0           layer_68_expanded_conv_5/squeeze_\n",
      "__________________________________________________________________________________________________\n",
      "layer_70_expanded_conv_5/squeez (None, 1, 1, 120)    3960        layer_69_expanded_conv_5/squeeze_\n",
      "__________________________________________________________________________________________________\n",
      "layer_71_activation_16 (Activat (None, 1, 1, 120)    0           layer_70_expanded_conv_5/squeeze_\n",
      "__________________________________________________________________________________________________\n",
      "layer_72_expanded_conv_5/squeez (None, 24, 32, 120)  0           layer_65_activation_15[0][0]     \n",
      "                                                                 layer_71_activation_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_73_expanded_conv_5/projec (None, 24, 32, 40)   4800        layer_72_expanded_conv_5/squeeze_\n",
      "__________________________________________________________________________________________________\n",
      "layer_74_expanded_conv_5/projec (None, 24, 32, 40)   160         layer_73_expanded_conv_5/project[\n",
      "__________________________________________________________________________________________________\n",
      "layer_75_expanded_conv_5/Add (A (None, 24, 32, 40)   0           layer_59_expanded_conv_4/Add[0][0\n",
      "                                                                 layer_74_expanded_conv_5/project/\n",
      "__________________________________________________________________________________________________\n",
      "layer_76_expanded_conv_6/expand (None, 24, 32, 240)  9600        layer_75_expanded_conv_5/Add[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_77_expanded_conv_6/expand (None, 24, 32, 240)  960         layer_76_expanded_conv_6/expand[0\n",
      "__________________________________________________________________________________________________\n",
      "layer_78_activation_17 (Activat (None, 24, 32, 240)  0           layer_77_expanded_conv_6/expand/B\n",
      "__________________________________________________________________________________________________\n",
      "layer_79_expanded_conv_6/depthw (None, 25, 33, 240)  0           layer_78_activation_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_80_expanded_conv_6/depthw (None, 12, 16, 240)  2160        layer_79_expanded_conv_6/depthwis\n",
      "__________________________________________________________________________________________________\n",
      "layer_81_expanded_conv_6/depthw (None, 12, 16, 240)  960         layer_80_expanded_conv_6/depthwis\n",
      "__________________________________________________________________________________________________\n",
      "layer_82_activation_19 (Activat (None, 12, 16, 240)  0           layer_81_expanded_conv_6/depthwis\n",
      "__________________________________________________________________________________________________\n",
      "layer_83_expanded_conv_6/projec (None, 12, 16, 80)   19200       layer_82_activation_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_84_expanded_conv_6/projec (None, 12, 16, 80)   320         layer_83_expanded_conv_6/project[\n",
      "__________________________________________________________________________________________________\n",
      "layer_85_expanded_conv_7/expand (None, 12, 16, 200)  16000       layer_84_expanded_conv_6/project/\n",
      "__________________________________________________________________________________________________\n",
      "layer_86_expanded_conv_7/expand (None, 12, 16, 200)  800         layer_85_expanded_conv_7/expand[0\n",
      "__________________________________________________________________________________________________\n",
      "layer_87_activation_21 (Activat (None, 12, 16, 200)  0           layer_86_expanded_conv_7/expand/B\n",
      "__________________________________________________________________________________________________\n",
      "layer_88_expanded_conv_7/depthw (None, 12, 16, 200)  1800        layer_87_activation_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_89_expanded_conv_7/depthw (None, 12, 16, 200)  800         layer_88_expanded_conv_7/depthwis\n",
      "__________________________________________________________________________________________________\n",
      "layer_90_activation_23 (Activat (None, 12, 16, 200)  0           layer_89_expanded_conv_7/depthwis\n",
      "__________________________________________________________________________________________________\n",
      "layer_91_expanded_conv_7/projec (None, 12, 16, 80)   16000       layer_90_activation_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_92_expanded_conv_7/projec (None, 12, 16, 80)   320         layer_91_expanded_conv_7/project[\n",
      "__________________________________________________________________________________________________\n",
      "layer_93_expanded_conv_7/Add (A (None, 12, 16, 80)   0           layer_84_expanded_conv_6/project/\n",
      "                                                                 layer_92_expanded_conv_7/project/\n",
      "__________________________________________________________________________________________________\n",
      "layer_94_expanded_conv_8/expand (None, 12, 16, 184)  14720       layer_93_expanded_conv_7/Add[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_95_expanded_conv_8/expand (None, 12, 16, 184)  736         layer_94_expanded_conv_8/expand[0\n",
      "__________________________________________________________________________________________________\n",
      "layer_96_activation_25 (Activat (None, 12, 16, 184)  0           layer_95_expanded_conv_8/expand/B\n",
      "__________________________________________________________________________________________________\n",
      "layer_97_expanded_conv_8/depthw (None, 12, 16, 184)  1656        layer_96_activation_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_98_expanded_conv_8/depthw (None, 12, 16, 184)  736         layer_97_expanded_conv_8/depthwis\n",
      "__________________________________________________________________________________________________\n",
      "layer_99_activation_27 (Activat (None, 12, 16, 184)  0           layer_98_expanded_conv_8/depthwis\n",
      "__________________________________________________________________________________________________\n",
      "layer_100_expanded_conv_8/proje (None, 12, 16, 80)   14720       layer_99_activation_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_101_expanded_conv_8/proje (None, 12, 16, 80)   320         layer_100_expanded_conv_8/project\n",
      "__________________________________________________________________________________________________\n",
      "layer_102_expanded_conv_8/Add ( (None, 12, 16, 80)   0           layer_93_expanded_conv_7/Add[0][0\n",
      "                                                                 layer_101_expanded_conv_8/project\n",
      "__________________________________________________________________________________________________\n",
      "layer_103_expanded_conv_9/expan (None, 12, 16, 184)  14720       layer_102_expanded_conv_8/Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_104_expanded_conv_9/expan (None, 12, 16, 184)  736         layer_103_expanded_conv_9/expand[\n",
      "__________________________________________________________________________________________________\n",
      "layer_105_activation_29 (Activa (None, 12, 16, 184)  0           layer_104_expanded_conv_9/expand/\n",
      "__________________________________________________________________________________________________\n",
      "layer_106_expanded_conv_9/depth (None, 12, 16, 184)  1656        layer_105_activation_29[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_107_expanded_conv_9/depth (None, 12, 16, 184)  736         layer_106_expanded_conv_9/depthwi\n",
      "__________________________________________________________________________________________________\n",
      "layer_108_activation_31 (Activa (None, 12, 16, 184)  0           layer_107_expanded_conv_9/depthwi\n",
      "__________________________________________________________________________________________________\n",
      "layer_109_expanded_conv_9/proje (None, 12, 16, 80)   14720       layer_108_activation_31[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_110_expanded_conv_9/proje (None, 12, 16, 80)   320         layer_109_expanded_conv_9/project\n",
      "__________________________________________________________________________________________________\n",
      "layer_111_expanded_conv_9/Add ( (None, 12, 16, 80)   0           layer_102_expanded_conv_8/Add[0][\n",
      "                                                                 layer_110_expanded_conv_9/project\n",
      "__________________________________________________________________________________________________\n",
      "layer_112_expanded_conv_10/expa (None, 12, 16, 480)  38400       layer_111_expanded_conv_9/Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_113_expanded_conv_10/expa (None, 12, 16, 480)  1920        layer_112_expanded_conv_10/expand\n",
      "__________________________________________________________________________________________________\n",
      "layer_114_activation_33 (Activa (None, 12, 16, 480)  0           layer_113_expanded_conv_10/expand\n",
      "__________________________________________________________________________________________________\n",
      "layer_115_expanded_conv_10/dept (None, 12, 16, 480)  4320        layer_114_activation_33[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_116_expanded_conv_10/dept (None, 12, 16, 480)  1920        layer_115_expanded_conv_10/depthw\n",
      "__________________________________________________________________________________________________\n",
      "layer_117_activation_35 (Activa (None, 12, 16, 480)  0           layer_116_expanded_conv_10/depthw\n",
      "__________________________________________________________________________________________________\n",
      "layer_118_expanded_conv_10/sque (None, 480)          0           layer_117_activation_35[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_119_reshape_4 (Reshape)   (None, 1, 1, 480)    0           layer_118_expanded_conv_10/squeez\n",
      "__________________________________________________________________________________________________\n",
      "layer_120_expanded_conv_10/sque (None, 1, 1, 120)    57720       layer_119_reshape_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_121_expanded_conv_10/sque (None, 1, 1, 120)    0           layer_120_expanded_conv_10/squeez\n",
      "__________________________________________________________________________________________________\n",
      "layer_122_expanded_conv_10/sque (None, 1, 1, 480)    58080       layer_121_expanded_conv_10/squeez\n",
      "__________________________________________________________________________________________________\n",
      "layer_123_activation_37 (Activa (None, 1, 1, 480)    0           layer_122_expanded_conv_10/squeez\n",
      "__________________________________________________________________________________________________\n",
      "layer_124_expanded_conv_10/sque (None, 12, 16, 480)  0           layer_117_activation_35[0][0]    \n",
      "                                                                 layer_123_activation_37[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_125_expanded_conv_10/proj (None, 12, 16, 112)  53760       layer_124_expanded_conv_10/squeez\n",
      "__________________________________________________________________________________________________\n",
      "layer_126_expanded_conv_10/proj (None, 12, 16, 112)  448         layer_125_expanded_conv_10/projec\n",
      "__________________________________________________________________________________________________\n",
      "layer_127_expanded_conv_11/expa (None, 12, 16, 672)  75264       layer_126_expanded_conv_10/projec\n",
      "__________________________________________________________________________________________________\n",
      "layer_128_expanded_conv_11/expa (None, 12, 16, 672)  2688        layer_127_expanded_conv_11/expand\n",
      "__________________________________________________________________________________________________\n",
      "layer_129_activation_38 (Activa (None, 12, 16, 672)  0           layer_128_expanded_conv_11/expand\n",
      "__________________________________________________________________________________________________\n",
      "layer_130_expanded_conv_11/dept (None, 12, 16, 672)  6048        layer_129_activation_38[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_131_expanded_conv_11/dept (None, 12, 16, 672)  2688        layer_130_expanded_conv_11/depthw\n",
      "__________________________________________________________________________________________________\n",
      "layer_132_activation_40 (Activa (None, 12, 16, 672)  0           layer_131_expanded_conv_11/depthw\n",
      "__________________________________________________________________________________________________\n",
      "layer_133_expanded_conv_11/sque (None, 672)          0           layer_132_activation_40[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_134_reshape_5 (Reshape)   (None, 1, 1, 672)    0           layer_133_expanded_conv_11/squeez\n",
      "__________________________________________________________________________________________________\n",
      "layer_135_expanded_conv_11/sque (None, 1, 1, 168)    113064      layer_134_reshape_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_136_expanded_conv_11/sque (None, 1, 1, 168)    0           layer_135_expanded_conv_11/squeez\n",
      "__________________________________________________________________________________________________\n",
      "layer_137_expanded_conv_11/sque (None, 1, 1, 672)    113568      layer_136_expanded_conv_11/squeez\n",
      "__________________________________________________________________________________________________\n",
      "layer_138_activation_42 (Activa (None, 1, 1, 672)    0           layer_137_expanded_conv_11/squeez\n",
      "__________________________________________________________________________________________________\n",
      "layer_139_expanded_conv_11/sque (None, 12, 16, 672)  0           layer_132_activation_40[0][0]    \n",
      "                                                                 layer_138_activation_42[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_140_expanded_conv_11/proj (None, 12, 16, 112)  75264       layer_139_expanded_conv_11/squeez\n",
      "__________________________________________________________________________________________________\n",
      "layer_141_expanded_conv_11/proj (None, 12, 16, 112)  448         layer_140_expanded_conv_11/projec\n",
      "__________________________________________________________________________________________________\n",
      "layer_142_expanded_conv_11/Add  (None, 12, 16, 112)  0           layer_126_expanded_conv_10/projec\n",
      "                                                                 layer_141_expanded_conv_11/projec\n",
      "__________________________________________________________________________________________________\n",
      "layer_143_expanded_conv_12/expa (None, 12, 16, 672)  75264       layer_142_expanded_conv_11/Add[0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_144_expanded_conv_12/expa (None, 12, 16, 672)  2688        layer_143_expanded_conv_12/expand\n",
      "__________________________________________________________________________________________________\n",
      "layer_145_activation_43 (Activa (None, 12, 16, 672)  0           layer_144_expanded_conv_12/expand\n",
      "__________________________________________________________________________________________________\n",
      "layer_146_expanded_conv_12/dept (None, 15, 19, 672)  0           layer_145_activation_43[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_147_expanded_conv_12/dept (None, 6, 8, 672)    16800       layer_146_expanded_conv_12/depthw\n",
      "__________________________________________________________________________________________________\n",
      "layer_148_expanded_conv_12/dept (None, 6, 8, 672)    2688        layer_147_expanded_conv_12/depthw\n",
      "__________________________________________________________________________________________________\n",
      "layer_149_activation_45 (Activa (None, 6, 8, 672)    0           layer_148_expanded_conv_12/depthw\n",
      "__________________________________________________________________________________________________\n",
      "layer_150_expanded_conv_12/sque (None, 672)          0           layer_149_activation_45[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_151_reshape_6 (Reshape)   (None, 1, 1, 672)    0           layer_150_expanded_conv_12/squeez\n",
      "__________________________________________________________________________________________________\n",
      "layer_152_expanded_conv_12/sque (None, 1, 1, 168)    113064      layer_151_reshape_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_153_expanded_conv_12/sque (None, 1, 1, 168)    0           layer_152_expanded_conv_12/squeez\n",
      "__________________________________________________________________________________________________\n",
      "layer_154_expanded_conv_12/sque (None, 1, 1, 672)    113568      layer_153_expanded_conv_12/squeez\n",
      "__________________________________________________________________________________________________\n",
      "layer_155_activation_47 (Activa (None, 1, 1, 672)    0           layer_154_expanded_conv_12/squeez\n",
      "__________________________________________________________________________________________________\n",
      "layer_156_expanded_conv_12/sque (None, 6, 8, 672)    0           layer_149_activation_45[0][0]    \n",
      "                                                                 layer_155_activation_47[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_157_expanded_conv_12/proj (None, 6, 8, 160)    107520      layer_156_expanded_conv_12/squeez\n",
      "__________________________________________________________________________________________________\n",
      "layer_158_expanded_conv_12/proj (None, 6, 8, 160)    640         layer_157_expanded_conv_12/projec\n",
      "__________________________________________________________________________________________________\n",
      "layer_159_expanded_conv_13/expa (None, 6, 8, 960)    153600      layer_158_expanded_conv_12/projec\n",
      "__________________________________________________________________________________________________\n",
      "layer_160_expanded_conv_13/expa (None, 6, 8, 960)    3840        layer_159_expanded_conv_13/expand\n",
      "__________________________________________________________________________________________________\n",
      "layer_161_activation_48 (Activa (None, 6, 8, 960)    0           layer_160_expanded_conv_13/expand\n",
      "__________________________________________________________________________________________________\n",
      "layer_162_expanded_conv_13/dept (None, 6, 8, 960)    24000       layer_161_activation_48[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_163_expanded_conv_13/dept (None, 6, 8, 960)    3840        layer_162_expanded_conv_13/depthw\n",
      "__________________________________________________________________________________________________\n",
      "layer_164_activation_50 (Activa (None, 6, 8, 960)    0           layer_163_expanded_conv_13/depthw\n",
      "__________________________________________________________________________________________________\n",
      "layer_165_expanded_conv_13/sque (None, 960)          0           layer_164_activation_50[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_166_reshape_7 (Reshape)   (None, 1, 1, 960)    0           layer_165_expanded_conv_13/squeez\n",
      "__________________________________________________________________________________________________\n",
      "layer_167_expanded_conv_13/sque (None, 1, 1, 240)    230640      layer_166_reshape_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_168_expanded_conv_13/sque (None, 1, 1, 240)    0           layer_167_expanded_conv_13/squeez\n",
      "__________________________________________________________________________________________________\n",
      "layer_169_expanded_conv_13/sque (None, 1, 1, 960)    231360      layer_168_expanded_conv_13/squeez\n",
      "__________________________________________________________________________________________________\n",
      "layer_170_activation_52 (Activa (None, 1, 1, 960)    0           layer_169_expanded_conv_13/squeez\n",
      "__________________________________________________________________________________________________\n",
      "layer_171_expanded_conv_13/sque (None, 6, 8, 960)    0           layer_164_activation_50[0][0]    \n",
      "                                                                 layer_170_activation_52[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_172_expanded_conv_13/proj (None, 6, 8, 160)    153600      layer_171_expanded_conv_13/squeez\n",
      "__________________________________________________________________________________________________\n",
      "layer_173_expanded_conv_13/proj (None, 6, 8, 160)    640         layer_172_expanded_conv_13/projec\n",
      "__________________________________________________________________________________________________\n",
      "layer_174_expanded_conv_13/Add  (None, 6, 8, 160)    0           layer_158_expanded_conv_12/projec\n",
      "                                                                 layer_173_expanded_conv_13/projec\n",
      "__________________________________________________________________________________________________\n",
      "layer_175_expanded_conv_14/expa (None, 6, 8, 960)    153600      layer_174_expanded_conv_13/Add[0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_176_expanded_conv_14/expa (None, 6, 8, 960)    3840        layer_175_expanded_conv_14/expand\n",
      "__________________________________________________________________________________________________\n",
      "layer_177_activation_53 (Activa (None, 6, 8, 960)    0           layer_176_expanded_conv_14/expand\n",
      "__________________________________________________________________________________________________\n",
      "layer_178_expanded_conv_14/dept (None, 6, 8, 960)    24000       layer_177_activation_53[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_179_expanded_conv_14/dept (None, 6, 8, 960)    3840        layer_178_expanded_conv_14/depthw\n",
      "__________________________________________________________________________________________________\n",
      "layer_180_activation_55 (Activa (None, 6, 8, 960)    0           layer_179_expanded_conv_14/depthw\n",
      "__________________________________________________________________________________________________\n",
      "layer_181_expanded_conv_14/sque (None, 960)          0           layer_180_activation_55[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_182_reshape_8 (Reshape)   (None, 1, 1, 960)    0           layer_181_expanded_conv_14/squeez\n",
      "__________________________________________________________________________________________________\n",
      "layer_183_expanded_conv_14/sque (None, 1, 1, 240)    230640      layer_182_reshape_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_184_expanded_conv_14/sque (None, 1, 1, 240)    0           layer_183_expanded_conv_14/squeez\n",
      "__________________________________________________________________________________________________\n",
      "layer_185_expanded_conv_14/sque (None, 1, 1, 960)    231360      layer_184_expanded_conv_14/squeez\n",
      "__________________________________________________________________________________________________\n",
      "layer_186_activation_57 (Activa (None, 1, 1, 960)    0           layer_185_expanded_conv_14/squeez\n",
      "__________________________________________________________________________________________________\n",
      "layer_187_expanded_conv_14/sque (None, 6, 8, 960)    0           layer_180_activation_55[0][0]    \n",
      "                                                                 layer_186_activation_57[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_188_expanded_conv_14/proj (None, 6, 8, 160)    153600      layer_187_expanded_conv_14/squeez\n",
      "__________________________________________________________________________________________________\n",
      "layer_189_expanded_conv_14/proj (None, 6, 8, 160)    640         layer_188_expanded_conv_14/projec\n",
      "__________________________________________________________________________________________________\n",
      "layer_190_expanded_conv_14/Add  (None, 6, 8, 160)    0           layer_174_expanded_conv_13/Add[0]\n",
      "                                                                 layer_189_expanded_conv_14/projec\n",
      "__________________________________________________________________________________________________\n",
      "layer_191_Conv_1 (Conv2D)       (None, 6, 8, 960)    153600      layer_190_expanded_conv_14/Add[0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_192_Conv_1/BatchNorm (Bat (None, 6, 8, 960)    3840        layer_191_Conv_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_193_activation_58 (Activa (None, 6, 8, 960)    0           layer_192_Conv_1/BatchNorm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0_upsampling (UpSa (None, 12, 16, 960)  0           layer_193_activation_58[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0_concat (Concaten (None, 12, 16, 1632) 0           decoder_stage0_upsampling[0][0]  \n",
      "                                                                 layer_145_activation_43[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0a_conv (Conv2D)   (None, 12, 16, 256)  3760128     decoder_stage0_concat[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0a_bn (BatchNormal (None, 12, 16, 256)  1024        decoder_stage0a_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0a_relu (Activatio (None, 12, 16, 256)  0           decoder_stage0a_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0b_conv (Conv2D)   (None, 12, 16, 256)  589824      decoder_stage0a_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0b_bn (BatchNormal (None, 12, 16, 256)  1024        decoder_stage0b_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0b_relu (Activatio (None, 12, 16, 256)  0           decoder_stage0b_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1_upsampling (UpSa (None, 24, 32, 256)  0           decoder_stage0b_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1_concat (Concaten (None, 24, 32, 496)  0           decoder_stage1_upsampling[0][0]  \n",
      "                                                                 layer_78_activation_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1a_conv (Conv2D)   (None, 24, 32, 128)  571392      decoder_stage1_concat[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1a_bn (BatchNormal (None, 24, 32, 128)  512         decoder_stage1a_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1a_relu (Activatio (None, 24, 32, 128)  0           decoder_stage1a_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1b_conv (Conv2D)   (None, 24, 32, 128)  147456      decoder_stage1a_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1b_bn (BatchNormal (None, 24, 32, 128)  512         decoder_stage1b_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1b_relu (Activatio (None, 24, 32, 128)  0           decoder_stage1b_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2_upsampling (UpSa (None, 48, 64, 128)  0           decoder_stage1b_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2_concat (Concaten (None, 48, 64, 200)  0           decoder_stage2_upsampling[0][0]  \n",
      "                                                                 layer_30_activation_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2a_conv (Conv2D)   (None, 48, 64, 64)   115200      decoder_stage2_concat[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2a_bn (BatchNormal (None, 48, 64, 64)   256         decoder_stage2a_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2a_relu (Activatio (None, 48, 64, 64)   0           decoder_stage2a_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2b_conv (Conv2D)   (None, 48, 64, 64)   36864       decoder_stage2a_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2b_bn (BatchNormal (None, 48, 64, 64)   256         decoder_stage2b_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2b_relu (Activatio (None, 48, 64, 64)   0           decoder_stage2b_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3_upsampling (UpSa (None, 96, 128, 64)  0           decoder_stage2b_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3_concat (Concaten (None, 96, 128, 128) 0           decoder_stage3_upsampling[0][0]  \n",
      "                                                                 layer_12_activation_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3a_conv (Conv2D)   (None, 96, 128, 32)  36864       decoder_stage3_concat[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3a_bn (BatchNormal (None, 96, 128, 32)  128         decoder_stage3a_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3a_relu (Activatio (None, 96, 128, 32)  0           decoder_stage3a_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3b_conv (Conv2D)   (None, 96, 128, 32)  9216        decoder_stage3a_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3b_bn (BatchNormal (None, 96, 128, 32)  128         decoder_stage3b_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3b_relu (Activatio (None, 96, 128, 32)  0           decoder_stage3b_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage4_upsampling (UpSa (None, 192, 256, 32) 0           decoder_stage3b_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage4a_conv (Conv2D)   (None, 192, 256, 16) 4608        decoder_stage4_upsampling[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage4a_bn (BatchNormal (None, 192, 256, 16) 64          decoder_stage4a_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage4a_relu (Activatio (None, 192, 256, 16) 0           decoder_stage4a_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage4b_conv (Conv2D)   (None, 192, 256, 16) 2304        decoder_stage4a_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage4b_bn (BatchNormal (None, 192, 256, 16) 64          decoder_stage4b_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage4b_relu (Activatio (None, 192, 256, 16) 0           decoder_stage4b_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 192, 256, 1)  145         decoder_stage4b_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 192, 256, 1)  0           conv2d_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 8,274,321\n",
      "Trainable params: 8,247,937\n",
      "Non-trainable params: 26,384\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for MBV3_UNet_nobottom_aug_swaConst20_jacc_0.001_200_192x256_yes\n",
      "Epoch 1/200\n",
      "250/250 [==============================] - 55s 221ms/step - loss: 0.3551 - iou_score: 0.6449 - f1-score: 0.7444 - precision: 0.7501 - recall: 0.8779 - val_loss: 0.5174 - val_iou_score: 0.3317 - val_f1-score: 0.4427 - val_precision: 0.7760 - val_recall: 0.3411\n",
      "Epoch 2/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.2425 - iou_score: 0.7575 - f1-score: 0.8401 - precision: 0.8570 - recall: 0.8897 - val_loss: 0.7247 - val_iou_score: 0.6381 - val_f1-score: 0.7439 - val_precision: 0.8420 - val_recall: 0.7656\n",
      "Epoch 3/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.2010 - iou_score: 0.7990 - f1-score: 0.8738 - precision: 0.8934 - recall: 0.8956 - val_loss: 0.4050 - val_iou_score: 0.5688 - val_f1-score: 0.6907 - val_precision: 0.9380 - val_recall: 0.5910\n",
      "Epoch 4/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1896 - iou_score: 0.8104 - f1-score: 0.8827 - precision: 0.9000 - recall: 0.9015 - val_loss: 0.1005 - val_iou_score: 0.7086 - val_f1-score: 0.8029 - val_precision: 0.8313 - val_recall: 0.8559\n",
      "Epoch 5/200\n",
      "250/250 [==============================] - 43s 172ms/step - loss: 0.1876 - iou_score: 0.8124 - f1-score: 0.8841 - precision: 0.9004 - recall: 0.9041 - val_loss: 0.2735 - val_iou_score: 0.7154 - val_f1-score: 0.8055 - val_precision: 0.9149 - val_recall: 0.7807\n",
      "Epoch 6/200\n",
      "250/250 [==============================] - 43s 170ms/step - loss: 0.1792 - iou_score: 0.8208 - f1-score: 0.8898 - precision: 0.9040 - recall: 0.9083 - val_loss: 0.4647 - val_iou_score: 0.6668 - val_f1-score: 0.7549 - val_precision: 0.8630 - val_recall: 0.7289\n",
      "Epoch 7/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1796 - iou_score: 0.8204 - f1-score: 0.8902 - precision: 0.9072 - recall: 0.9048 - val_loss: 0.1827 - val_iou_score: 0.7288 - val_f1-score: 0.8172 - val_precision: 0.8806 - val_recall: 0.8153\n",
      "Epoch 8/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1737 - iou_score: 0.8263 - f1-score: 0.8956 - precision: 0.9125 - recall: 0.9069 - val_loss: 0.2005 - val_iou_score: 0.7262 - val_f1-score: 0.8074 - val_precision: 0.8211 - val_recall: 0.8958\n",
      "Epoch 9/200\n",
      "250/250 [==============================] - 43s 170ms/step - loss: 0.1705 - iou_score: 0.8295 - f1-score: 0.8972 - precision: 0.9112 - recall: 0.9098 - val_loss: 0.1775 - val_iou_score: 0.7636 - val_f1-score: 0.8441 - val_precision: 0.8835 - val_recall: 0.8735\n",
      "Epoch 10/200\n",
      "250/250 [==============================] - 43s 170ms/step - loss: 0.1701 - iou_score: 0.8299 - f1-score: 0.8980 - precision: 0.9124 - recall: 0.9109 - val_loss: 0.5153 - val_iou_score: 0.7185 - val_f1-score: 0.8051 - val_precision: 0.8914 - val_recall: 0.7948\n",
      "Epoch 11/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1692 - iou_score: 0.8308 - f1-score: 0.8975 - precision: 0.9136 - recall: 0.9095 - val_loss: 0.1627 - val_iou_score: 0.7380 - val_f1-score: 0.8226 - val_precision: 0.8068 - val_recall: 0.9158\n",
      "Epoch 12/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1649 - iou_score: 0.8351 - f1-score: 0.9017 - precision: 0.9186 - recall: 0.9090 - val_loss: 0.4310 - val_iou_score: 0.7119 - val_f1-score: 0.7925 - val_precision: 0.8258 - val_recall: 0.8775\n",
      "Epoch 13/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1621 - iou_score: 0.8379 - f1-score: 0.9032 - precision: 0.9192 - recall: 0.9113 - val_loss: 0.3821 - val_iou_score: 0.6460 - val_f1-score: 0.7549 - val_precision: 0.6845 - val_recall: 0.9460\n",
      "Epoch 14/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1696 - iou_score: 0.8304 - f1-score: 0.8987 - precision: 0.9177 - recall: 0.9060 - val_loss: 0.2613 - val_iou_score: 0.7149 - val_f1-score: 0.8115 - val_precision: 0.8236 - val_recall: 0.8675\n",
      "Epoch 15/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1697 - iou_score: 0.8303 - f1-score: 0.8969 - precision: 0.9174 - recall: 0.9042 - val_loss: 0.2682 - val_iou_score: 0.7422 - val_f1-score: 0.8257 - val_precision: 0.8631 - val_recall: 0.8678\n",
      "Epoch 16/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1612 - iou_score: 0.8388 - f1-score: 0.9036 - precision: 0.9199 - recall: 0.9115 - val_loss: 0.2110 - val_iou_score: 0.7613 - val_f1-score: 0.8444 - val_precision: 0.8433 - val_recall: 0.9093\n",
      "Epoch 17/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1599 - iou_score: 0.8401 - f1-score: 0.9048 - precision: 0.9195 - recall: 0.9132 - val_loss: 0.4188 - val_iou_score: 0.7359 - val_f1-score: 0.8207 - val_precision: 0.8371 - val_recall: 0.8900\n",
      "Epoch 18/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1564 - iou_score: 0.8436 - f1-score: 0.9078 - precision: 0.9233 - recall: 0.9141 - val_loss: 0.2407 - val_iou_score: 0.7135 - val_f1-score: 0.8043 - val_precision: 0.8368 - val_recall: 0.8531\n",
      "Epoch 19/200\n",
      "250/250 [==============================] - 43s 170ms/step - loss: 0.1605 - iou_score: 0.8395 - f1-score: 0.9044 - precision: 0.9195 - recall: 0.9126 - val_loss: 0.2542 - val_iou_score: 0.7484 - val_f1-score: 0.8382 - val_precision: 0.8442 - val_recall: 0.8959\n",
      "Epoch 20/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1539 - iou_score: 0.8461 - f1-score: 0.9093 - precision: 0.9228 - recall: 0.9154 - val_loss: 0.2631 - val_iou_score: 0.7398 - val_f1-score: 0.8299 - val_precision: 0.8194 - val_recall: 0.9074\n",
      "Epoch 21/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1541 - iou_score: 0.8459 - f1-score: 0.9093 - precision: 0.9237 - recall: 0.9168 - val_loss: 0.1588 - val_iou_score: 0.7804 - val_f1-score: 0.8607 - val_precision: 0.8337 - val_recall: 0.9415\n",
      "Epoch 22/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1576 - iou_score: 0.8424 - f1-score: 0.9064 - precision: 0.9206 - recall: 0.9151 - val_loss: 0.2704 - val_iou_score: 0.7483 - val_f1-score: 0.8341 - val_precision: 0.8363 - val_recall: 0.8943\n",
      "Epoch 23/200\n",
      "250/250 [==============================] - 43s 170ms/step - loss: 0.1642 - iou_score: 0.8358 - f1-score: 0.9014 - precision: 0.9191 - recall: 0.9094 - val_loss: 0.4340 - val_iou_score: 0.7481 - val_f1-score: 0.8281 - val_precision: 0.8398 - val_recall: 0.9036\n",
      "Epoch 24/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1575 - iou_score: 0.8425 - f1-score: 0.9069 - precision: 0.9206 - recall: 0.9158 - val_loss: 0.3521 - val_iou_score: 0.7416 - val_f1-score: 0.8321 - val_precision: 0.8064 - val_recall: 0.9244\n",
      "Epoch 25/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1558 - iou_score: 0.8442 - f1-score: 0.9079 - precision: 0.9237 - recall: 0.9137 - val_loss: 0.2787 - val_iou_score: 0.6694 - val_f1-score: 0.7858 - val_precision: 0.7562 - val_recall: 0.9047\n",
      "Epoch 26/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1554 - iou_score: 0.8446 - f1-score: 0.9087 - precision: 0.9222 - recall: 0.9164 - val_loss: 0.1749 - val_iou_score: 0.7315 - val_f1-score: 0.8155 - val_precision: 0.8201 - val_recall: 0.8605\n",
      "Epoch 27/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1551 - iou_score: 0.8449 - f1-score: 0.9082 - precision: 0.9236 - recall: 0.9140 - val_loss: 0.3853 - val_iou_score: 0.7711 - val_f1-score: 0.8519 - val_precision: 0.8857 - val_recall: 0.8700\n",
      "Epoch 28/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1512 - iou_score: 0.8488 - f1-score: 0.9113 - precision: 0.9259 - recall: 0.9168 - val_loss: 0.1658 - val_iou_score: 0.7959 - val_f1-score: 0.8746 - val_precision: 0.8952 - val_recall: 0.8943\n",
      "Epoch 29/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1498 - iou_score: 0.8502 - f1-score: 0.9122 - precision: 0.9242 - recall: 0.9210 - val_loss: 0.1009 - val_iou_score: 0.7684 - val_f1-score: 0.8493 - val_precision: 0.8562 - val_recall: 0.8904\n",
      "Epoch 30/200\n",
      "250/250 [==============================] - 43s 170ms/step - loss: 0.1474 - iou_score: 0.8526 - f1-score: 0.9143 - precision: 0.9272 - recall: 0.9204 - val_loss: 0.2195 - val_iou_score: 0.7650 - val_f1-score: 0.8434 - val_precision: 0.8646 - val_recall: 0.8714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1557 - iou_score: 0.8443 - f1-score: 0.9084 - precision: 0.9246 - recall: 0.9144 - val_loss: 0.1808 - val_iou_score: 0.7740 - val_f1-score: 0.8532 - val_precision: 0.8913 - val_recall: 0.8779\n",
      "Epoch 32/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1548 - iou_score: 0.8452 - f1-score: 0.9081 - precision: 0.9209 - recall: 0.9152 - val_loss: 0.1155 - val_iou_score: 0.7051 - val_f1-score: 0.7958 - val_precision: 0.7823 - val_recall: 0.9168\n",
      "Epoch 33/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1616 - iou_score: 0.8384 - f1-score: 0.9023 - precision: 0.9177 - recall: 0.9121 - val_loss: 0.2303 - val_iou_score: 0.7581 - val_f1-score: 0.8392 - val_precision: 0.8667 - val_recall: 0.8759\n",
      "Epoch 34/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1601 - iou_score: 0.8399 - f1-score: 0.9043 - precision: 0.9207 - recall: 0.9128 - val_loss: 0.1408 - val_iou_score: 0.7604 - val_f1-score: 0.8418 - val_precision: 0.8352 - val_recall: 0.9188\n",
      "Epoch 35/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1525 - iou_score: 0.8475 - f1-score: 0.9098 - precision: 0.9210 - recall: 0.9205 - val_loss: 0.3192 - val_iou_score: 0.6851 - val_f1-score: 0.7657 - val_precision: 0.7367 - val_recall: 0.9289\n",
      "Epoch 36/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1504 - iou_score: 0.8496 - f1-score: 0.9121 - precision: 0.9273 - recall: 0.9166 - val_loss: 0.1532 - val_iou_score: 0.7502 - val_f1-score: 0.8315 - val_precision: 0.8147 - val_recall: 0.9289\n",
      "Epoch 37/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1496 - iou_score: 0.8504 - f1-score: 0.9121 - precision: 0.9241 - recall: 0.9201 - val_loss: 0.3707 - val_iou_score: 0.7582 - val_f1-score: 0.8362 - val_precision: 0.8453 - val_recall: 0.9080\n",
      "Epoch 38/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1441 - iou_score: 0.8559 - f1-score: 0.9168 - precision: 0.9294 - recall: 0.9222 - val_loss: 0.3476 - val_iou_score: 0.7723 - val_f1-score: 0.8541 - val_precision: 0.9074 - val_recall: 0.8572\n",
      "Epoch 39/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1465 - iou_score: 0.8535 - f1-score: 0.9148 - precision: 0.9280 - recall: 0.9202 - val_loss: 0.2059 - val_iou_score: 0.7591 - val_f1-score: 0.8462 - val_precision: 0.8279 - val_recall: 0.9233\n",
      "Epoch 40/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1442 - iou_score: 0.8558 - f1-score: 0.9164 - precision: 0.9296 - recall: 0.9209 - val_loss: 0.2773 - val_iou_score: 0.7246 - val_f1-score: 0.8099 - val_precision: 0.8049 - val_recall: 0.9118\n",
      "Epoch 41/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1448 - iou_score: 0.8552 - f1-score: 0.9161 - precision: 0.9285 - recall: 0.9216 - val_loss: 0.1692 - val_iou_score: 0.7857 - val_f1-score: 0.8578 - val_precision: 0.8511 - val_recall: 0.9308\n",
      "Epoch 42/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1440 - iou_score: 0.8560 - f1-score: 0.9166 - precision: 0.9286 - recall: 0.9221 - val_loss: 0.3426 - val_iou_score: 0.7741 - val_f1-score: 0.8512 - val_precision: 0.8815 - val_recall: 0.8831\n",
      "Epoch 43/200\n",
      "250/250 [==============================] - 42s 168ms/step - loss: 0.1471 - iou_score: 0.8529 - f1-score: 0.9142 - precision: 0.9274 - recall: 0.9200 - val_loss: 0.1123 - val_iou_score: 0.6901 - val_f1-score: 0.7866 - val_precision: 0.7204 - val_recall: 0.9634\n",
      "Epoch 44/200\n",
      "250/250 [==============================] - 43s 170ms/step - loss: 0.1463 - iou_score: 0.8537 - f1-score: 0.9150 - precision: 0.9260 - recall: 0.9217 - val_loss: 0.1639 - val_iou_score: 0.7728 - val_f1-score: 0.8509 - val_precision: 0.8580 - val_recall: 0.9099\n",
      "Epoch 45/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1461 - iou_score: 0.8539 - f1-score: 0.9146 - precision: 0.9286 - recall: 0.9197 - val_loss: 0.1886 - val_iou_score: 0.7843 - val_f1-score: 0.8645 - val_precision: 0.8904 - val_recall: 0.8885\n",
      "Epoch 46/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1439 - iou_score: 0.8561 - f1-score: 0.9166 - precision: 0.9287 - recall: 0.9228 - val_loss: 0.1654 - val_iou_score: 0.7997 - val_f1-score: 0.8754 - val_precision: 0.8715 - val_recall: 0.9230\n",
      "Epoch 47/200\n",
      "250/250 [==============================] - 42s 168ms/step - loss: 0.1416 - iou_score: 0.8584 - f1-score: 0.9180 - precision: 0.9298 - recall: 0.9222 - val_loss: 0.2332 - val_iou_score: 0.7737 - val_f1-score: 0.8524 - val_precision: 0.8733 - val_recall: 0.8934\n",
      "Epoch 48/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1545 - iou_score: 0.8455 - f1-score: 0.9088 - precision: 0.9232 - recall: 0.9162 - val_loss: 0.3553 - val_iou_score: 0.7238 - val_f1-score: 0.8147 - val_precision: 0.8045 - val_recall: 0.8989\n",
      "Epoch 49/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1506 - iou_score: 0.8494 - f1-score: 0.9119 - precision: 0.9271 - recall: 0.9167 - val_loss: 0.3867 - val_iou_score: 0.7454 - val_f1-score: 0.8266 - val_precision: 0.8586 - val_recall: 0.8465\n",
      "Epoch 50/200\n",
      "250/250 [==============================] - 42s 168ms/step - loss: 0.1511 - iou_score: 0.8489 - f1-score: 0.9109 - precision: 0.9251 - recall: 0.9179 - val_loss: 0.1600 - val_iou_score: 0.7584 - val_f1-score: 0.8395 - val_precision: 0.8263 - val_recall: 0.9231\n",
      "Epoch 51/200\n",
      "250/250 [==============================] - 42s 168ms/step - loss: 0.1445 - iou_score: 0.8555 - f1-score: 0.9160 - precision: 0.9294 - recall: 0.9207 - val_loss: 0.1331 - val_iou_score: 0.7606 - val_f1-score: 0.8353 - val_precision: 0.8810 - val_recall: 0.8350\n",
      "Epoch 52/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1456 - iou_score: 0.8544 - f1-score: 0.9147 - precision: 0.9285 - recall: 0.9199 - val_loss: 0.3784 - val_iou_score: 0.7725 - val_f1-score: 0.8500 - val_precision: 0.8998 - val_recall: 0.8548\n",
      "Epoch 53/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1432 - iou_score: 0.8568 - f1-score: 0.9170 - precision: 0.9297 - recall: 0.9221 - val_loss: 0.1587 - val_iou_score: 0.7893 - val_f1-score: 0.8685 - val_precision: 0.8636 - val_recall: 0.9180\n",
      "Epoch 54/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1409 - iou_score: 0.8591 - f1-score: 0.9185 - precision: 0.9321 - recall: 0.9224 - val_loss: 0.0955 - val_iou_score: 0.7587 - val_f1-score: 0.8426 - val_precision: 0.8587 - val_recall: 0.8902\n",
      "Epoch 55/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1437 - iou_score: 0.8563 - f1-score: 0.9160 - precision: 0.9309 - recall: 0.9201 - val_loss: 0.2749 - val_iou_score: 0.7756 - val_f1-score: 0.8544 - val_precision: 0.8424 - val_recall: 0.9267\n",
      "Epoch 56/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1432 - iou_score: 0.8568 - f1-score: 0.9168 - precision: 0.9292 - recall: 0.9225 - val_loss: 0.1808 - val_iou_score: 0.7919 - val_f1-score: 0.8708 - val_precision: 0.8659 - val_recall: 0.9184\n",
      "Epoch 57/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1363 - iou_score: 0.8637 - f1-score: 0.9219 - precision: 0.9329 - recall: 0.9264 - val_loss: 0.1178 - val_iou_score: 0.7571 - val_f1-score: 0.8447 - val_precision: 0.8447 - val_recall: 0.9000\n",
      "Epoch 58/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1414 - iou_score: 0.8586 - f1-score: 0.9183 - precision: 0.9299 - recall: 0.9237 - val_loss: 0.2633 - val_iou_score: 0.7622 - val_f1-score: 0.8419 - val_precision: 0.8253 - val_recall: 0.9195\n",
      "Epoch 59/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1402 - iou_score: 0.8598 - f1-score: 0.9191 - precision: 0.9327 - recall: 0.9222 - val_loss: 0.1114 - val_iou_score: 0.7856 - val_f1-score: 0.8618 - val_precision: 0.8871 - val_recall: 0.8880\n",
      "Epoch 60/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1398 - iou_score: 0.8602 - f1-score: 0.9200 - precision: 0.9323 - recall: 0.9235 - val_loss: 0.1680 - val_iou_score: 0.8093 - val_f1-score: 0.8837 - val_precision: 0.8928 - val_recall: 0.9102\n",
      "Epoch 61/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1364 - iou_score: 0.8636 - f1-score: 0.9217 - precision: 0.9329 - recall: 0.9266 - val_loss: 0.1568 - val_iou_score: 0.7400 - val_f1-score: 0.8253 - val_precision: 0.8006 - val_recall: 0.9098\n",
      "Epoch 62/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1369 - iou_score: 0.8631 - f1-score: 0.9217 - precision: 0.9320 - recall: 0.9265 - val_loss: 0.4423 - val_iou_score: 0.7266 - val_f1-score: 0.8082 - val_precision: 0.7917 - val_recall: 0.9225\n",
      "Epoch 63/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1384 - iou_score: 0.8616 - f1-score: 0.9201 - precision: 0.9300 - recall: 0.9259 - val_loss: 0.3513 - val_iou_score: 0.7987 - val_f1-score: 0.8720 - val_precision: 0.8986 - val_recall: 0.8906\n",
      "Epoch 64/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1351 - iou_score: 0.8649 - f1-score: 0.9229 - precision: 0.9332 - recall: 0.9276 - val_loss: 0.1501 - val_iou_score: 0.8011 - val_f1-score: 0.8763 - val_precision: 0.8793 - val_recall: 0.9173\n",
      "Epoch 65/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1475 - iou_score: 0.8525 - f1-score: 0.9137 - precision: 0.9297 - recall: 0.9168 - val_loss: 0.3907 - val_iou_score: 0.6367 - val_f1-score: 0.7297 - val_precision: 0.6737 - val_recall: 0.9481\n",
      "Epoch 66/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1404 - iou_score: 0.8596 - f1-score: 0.9190 - precision: 0.9297 - recall: 0.9245 - val_loss: 0.1486 - val_iou_score: 0.7895 - val_f1-score: 0.8636 - val_precision: 0.8795 - val_recall: 0.9046\n",
      "Epoch 67/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1526 - iou_score: 0.8474 - f1-score: 0.9096 - precision: 0.9246 - recall: 0.9166 - val_loss: 0.3626 - val_iou_score: 0.7496 - val_f1-score: 0.8382 - val_precision: 0.8353 - val_recall: 0.8998\n",
      "Epoch 68/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1407 - iou_score: 0.8593 - f1-score: 0.9191 - precision: 0.9297 - recall: 0.9247 - val_loss: 0.1555 - val_iou_score: 0.7657 - val_f1-score: 0.8450 - val_precision: 0.8581 - val_recall: 0.8985\n",
      "Epoch 69/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1391 - iou_score: 0.8609 - f1-score: 0.9197 - precision: 0.9321 - recall: 0.9235 - val_loss: 0.1650 - val_iou_score: 0.7695 - val_f1-score: 0.8482 - val_precision: 0.8461 - val_recall: 0.9156\n",
      "Epoch 70/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1402 - iou_score: 0.8598 - f1-score: 0.9187 - precision: 0.9294 - recall: 0.9255 - val_loss: 0.1846 - val_iou_score: 0.7878 - val_f1-score: 0.8653 - val_precision: 0.8738 - val_recall: 0.9056\n",
      "Epoch 71/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1393 - iou_score: 0.8607 - f1-score: 0.9194 - precision: 0.9316 - recall: 0.9238 - val_loss: 0.1789 - val_iou_score: 0.7905 - val_f1-score: 0.8646 - val_precision: 0.8643 - val_recall: 0.9208\n",
      "Epoch 72/200\n",
      "250/250 [==============================] - 42s 168ms/step - loss: 0.1354 - iou_score: 0.8646 - f1-score: 0.9224 - precision: 0.9313 - recall: 0.9290 - val_loss: 0.2755 - val_iou_score: 0.7554 - val_f1-score: 0.8401 - val_precision: 0.8468 - val_recall: 0.9015\n",
      "Epoch 73/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1367 - iou_score: 0.8633 - f1-score: 0.9212 - precision: 0.9314 - recall: 0.9275 - val_loss: 0.3340 - val_iou_score: 0.7761 - val_f1-score: 0.8548 - val_precision: 0.8617 - val_recall: 0.9094\n",
      "Epoch 74/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1378 - iou_score: 0.8622 - f1-score: 0.9207 - precision: 0.9312 - recall: 0.9261 - val_loss: 0.3879 - val_iou_score: 0.7857 - val_f1-score: 0.8665 - val_precision: 0.8897 - val_recall: 0.8893\n",
      "Epoch 75/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1360 - iou_score: 0.8640 - f1-score: 0.9221 - precision: 0.9341 - recall: 0.9250 - val_loss: 0.1443 - val_iou_score: 0.7872 - val_f1-score: 0.8665 - val_precision: 0.8757 - val_recall: 0.9064\n",
      "Epoch 76/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1334 - iou_score: 0.8666 - f1-score: 0.9238 - precision: 0.9352 - recall: 0.9270 - val_loss: 0.1426 - val_iou_score: 0.7812 - val_f1-score: 0.8601 - val_precision: 0.8666 - val_recall: 0.9092\n",
      "Epoch 77/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1318 - iou_score: 0.8682 - f1-score: 0.9247 - precision: 0.9359 - recall: 0.9282 - val_loss: 0.3894 - val_iou_score: 0.7744 - val_f1-score: 0.8547 - val_precision: 0.8536 - val_recall: 0.9141\n",
      "Epoch 78/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1331 - iou_score: 0.8669 - f1-score: 0.9242 - precision: 0.9345 - recall: 0.9280 - val_loss: 0.1577 - val_iou_score: 0.8162 - val_f1-score: 0.8859 - val_precision: 0.8965 - val_recall: 0.9147\n",
      "Epoch 79/200\n",
      "250/250 [==============================] - 42s 168ms/step - loss: 0.1336 - iou_score: 0.8664 - f1-score: 0.9239 - precision: 0.9337 - recall: 0.9281 - val_loss: 0.1143 - val_iou_score: 0.7631 - val_f1-score: 0.8455 - val_precision: 0.8402 - val_recall: 0.9165\n",
      "Epoch 80/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1315 - iou_score: 0.8685 - f1-score: 0.9251 - precision: 0.9325 - recall: 0.9306 - val_loss: 0.2755 - val_iou_score: 0.7611 - val_f1-score: 0.8415 - val_precision: 0.8839 - val_recall: 0.8567\n",
      "Epoch 81/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1355 - iou_score: 0.8645 - f1-score: 0.9220 - precision: 0.9335 - recall: 0.9262 - val_loss: 0.1733 - val_iou_score: 0.7789 - val_f1-score: 0.8599 - val_precision: 0.8849 - val_recall: 0.8865\n",
      "Epoch 82/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1426 - iou_score: 0.8574 - f1-score: 0.9172 - precision: 0.9296 - recall: 0.9221 - val_loss: 0.1319 - val_iou_score: 0.7527 - val_f1-score: 0.8344 - val_precision: 0.8182 - val_recall: 0.9281\n",
      "Epoch 83/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1349 - iou_score: 0.8651 - f1-score: 0.9233 - precision: 0.9335 - recall: 0.9265 - val_loss: 0.2443 - val_iou_score: 0.7682 - val_f1-score: 0.8497 - val_precision: 0.8440 - val_recall: 0.9152\n",
      "Epoch 84/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1377 - iou_score: 0.8623 - f1-score: 0.9209 - precision: 0.9305 - recall: 0.9270 - val_loss: 0.1180 - val_iou_score: 0.7916 - val_f1-score: 0.8641 - val_precision: 0.8929 - val_recall: 0.8847\n",
      "Epoch 85/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1316 - iou_score: 0.8684 - f1-score: 0.9250 - precision: 0.9361 - recall: 0.9273 - val_loss: 0.1628 - val_iou_score: 0.8047 - val_f1-score: 0.8833 - val_precision: 0.9171 - val_recall: 0.8809\n",
      "Epoch 86/200\n",
      "250/250 [==============================] - 42s 168ms/step - loss: 0.1324 - iou_score: 0.8676 - f1-score: 0.9247 - precision: 0.9369 - recall: 0.9268 - val_loss: 0.3790 - val_iou_score: 0.7281 - val_f1-score: 0.8115 - val_precision: 0.8706 - val_recall: 0.8244\n",
      "Epoch 87/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1311 - iou_score: 0.8689 - f1-score: 0.9261 - precision: 0.9372 - recall: 0.9279 - val_loss: 0.3331 - val_iou_score: 0.7805 - val_f1-score: 0.8590 - val_precision: 0.8907 - val_recall: 0.8714\n",
      "Epoch 88/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1347 - iou_score: 0.8653 - f1-score: 0.9220 - precision: 0.9298 - recall: 0.9304 - val_loss: 0.3827 - val_iou_score: 0.7608 - val_f1-score: 0.8431 - val_precision: 0.8104 - val_recall: 0.9443\n",
      "Epoch 89/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1342 - iou_score: 0.8658 - f1-score: 0.9234 - precision: 0.9336 - recall: 0.9279 - val_loss: 0.1687 - val_iou_score: 0.7809 - val_f1-score: 0.8613 - val_precision: 0.8706 - val_recall: 0.9043\n",
      "Epoch 90/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1294 - iou_score: 0.8706 - f1-score: 0.9268 - precision: 0.9351 - recall: 0.9314 - val_loss: 0.2837 - val_iou_score: 0.7416 - val_f1-score: 0.8267 - val_precision: 0.8120 - val_recall: 0.9222\n",
      "Epoch 91/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1294 - iou_score: 0.8706 - f1-score: 0.9266 - precision: 0.9357 - recall: 0.9303 - val_loss: 0.2087 - val_iou_score: 0.7670 - val_f1-score: 0.8499 - val_precision: 0.8147 - val_recall: 0.9468\n",
      "Epoch 92/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1300 - iou_score: 0.8700 - f1-score: 0.9260 - precision: 0.9346 - recall: 0.9317 - val_loss: 0.4025 - val_iou_score: 0.7888 - val_f1-score: 0.8628 - val_precision: 0.8572 - val_recall: 0.9274\n",
      "Epoch 93/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1283 - iou_score: 0.8717 - f1-score: 0.9277 - precision: 0.9353 - recall: 0.9324 - val_loss: 0.1220 - val_iou_score: 0.7813 - val_f1-score: 0.8579 - val_precision: 0.8538 - val_recall: 0.9222\n",
      "Epoch 94/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1293 - iou_score: 0.8707 - f1-score: 0.9268 - precision: 0.9375 - recall: 0.9296 - val_loss: 0.1627 - val_iou_score: 0.7740 - val_f1-score: 0.8530 - val_precision: 0.8533 - val_recall: 0.9126\n",
      "Epoch 95/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1269 - iou_score: 0.8731 - f1-score: 0.9284 - precision: 0.9371 - recall: 0.9324 - val_loss: 0.1978 - val_iou_score: 0.7920 - val_f1-score: 0.8677 - val_precision: 0.8872 - val_recall: 0.8991\n",
      "Epoch 96/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1294 - iou_score: 0.8706 - f1-score: 0.9267 - precision: 0.9350 - recall: 0.9311 - val_loss: 0.1717 - val_iou_score: 0.7888 - val_f1-score: 0.8649 - val_precision: 0.8715 - val_recall: 0.9117\n",
      "Epoch 97/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1323 - iou_score: 0.8677 - f1-score: 0.9244 - precision: 0.9347 - recall: 0.9292 - val_loss: 0.2369 - val_iou_score: 0.7233 - val_f1-score: 0.8092 - val_precision: 0.7848 - val_recall: 0.9329\n",
      "Epoch 98/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1262 - iou_score: 0.8738 - f1-score: 0.9291 - precision: 0.9364 - recall: 0.9338 - val_loss: 0.1855 - val_iou_score: 0.7889 - val_f1-score: 0.8607 - val_precision: 0.9157 - val_recall: 0.8552\n",
      "Epoch 99/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1268 - iou_score: 0.8732 - f1-score: 0.9286 - precision: 0.9366 - recall: 0.9324 - val_loss: 0.4038 - val_iou_score: 0.7820 - val_f1-score: 0.8604 - val_precision: 0.9043 - val_recall: 0.8602\n",
      "Epoch 100/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1263 - iou_score: 0.8737 - f1-score: 0.9286 - precision: 0.9384 - recall: 0.9316 - val_loss: 0.1905 - val_iou_score: 0.7401 - val_f1-score: 0.8292 - val_precision: 0.7997 - val_recall: 0.9343\n",
      "Epoch 101/200\n",
      "250/250 [==============================] - 42s 168ms/step - loss: 0.1286 - iou_score: 0.8714 - f1-score: 0.9265 - precision: 0.9343 - recall: 0.9331 - val_loss: 0.2060 - val_iou_score: 0.7310 - val_f1-score: 0.8149 - val_precision: 0.7869 - val_recall: 0.9334\n",
      "Epoch 102/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1270 - iou_score: 0.8730 - f1-score: 0.9286 - precision: 0.9376 - recall: 0.9311 - val_loss: 0.4288 - val_iou_score: 0.7658 - val_f1-score: 0.8465 - val_precision: 0.8438 - val_recall: 0.9135\n",
      "Epoch 103/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1278 - iou_score: 0.8722 - f1-score: 0.9276 - precision: 0.9350 - recall: 0.9334 - val_loss: 0.1502 - val_iou_score: 0.8162 - val_f1-score: 0.8908 - val_precision: 0.8830 - val_recall: 0.9290\n",
      "Epoch 104/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1314 - iou_score: 0.8686 - f1-score: 0.9246 - precision: 0.9341 - recall: 0.9294 - val_loss: 0.0975 - val_iou_score: 0.7306 - val_f1-score: 0.8156 - val_precision: 0.7770 - val_recall: 0.9457\n",
      "Epoch 105/200\n",
      "250/250 [==============================] - 43s 170ms/step - loss: 0.1256 - iou_score: 0.8744 - f1-score: 0.9292 - precision: 0.9361 - recall: 0.9337 - val_loss: 0.1691 - val_iou_score: 0.7590 - val_f1-score: 0.8360 - val_precision: 0.8279 - val_recall: 0.9279\n",
      "Epoch 106/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1236 - iou_score: 0.8764 - f1-score: 0.9306 - precision: 0.9388 - recall: 0.9337 - val_loss: 0.1603 - val_iou_score: 0.7826 - val_f1-score: 0.8636 - val_precision: 0.8620 - val_recall: 0.9167\n",
      "Epoch 107/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1249 - iou_score: 0.8751 - f1-score: 0.9295 - precision: 0.9392 - recall: 0.9309 - val_loss: 0.1241 - val_iou_score: 0.7848 - val_f1-score: 0.8608 - val_precision: 0.8867 - val_recall: 0.8931\n",
      "Epoch 108/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1281 - iou_score: 0.8719 - f1-score: 0.9275 - precision: 0.9363 - recall: 0.9308 - val_loss: 0.2295 - val_iou_score: 0.7652 - val_f1-score: 0.8464 - val_precision: 0.8528 - val_recall: 0.9065\n",
      "Epoch 109/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1257 - iou_score: 0.8743 - f1-score: 0.9288 - precision: 0.9348 - recall: 0.9347 - val_loss: 0.1060 - val_iou_score: 0.7838 - val_f1-score: 0.8597 - val_precision: 0.9005 - val_recall: 0.8777\n",
      "Epoch 110/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1235 - iou_score: 0.8765 - f1-score: 0.9306 - precision: 0.9377 - recall: 0.9341 - val_loss: 0.1589 - val_iou_score: 0.8068 - val_f1-score: 0.8824 - val_precision: 0.9114 - val_recall: 0.8890\n",
      "Epoch 111/200\n",
      "250/250 [==============================] - 42s 168ms/step - loss: 0.1254 - iou_score: 0.8746 - f1-score: 0.9293 - precision: 0.9383 - recall: 0.9320 - val_loss: 0.2768 - val_iou_score: 0.7315 - val_f1-score: 0.8228 - val_precision: 0.7826 - val_recall: 0.9432\n",
      "Epoch 112/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1257 - iou_score: 0.8743 - f1-score: 0.9288 - precision: 0.9386 - recall: 0.9313 - val_loss: 0.3405 - val_iou_score: 0.7607 - val_f1-score: 0.8395 - val_precision: 0.8229 - val_recall: 0.9328\n",
      "Epoch 113/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1232 - iou_score: 0.8768 - f1-score: 0.9304 - precision: 0.9381 - recall: 0.9345 - val_loss: 0.3085 - val_iou_score: 0.7873 - val_f1-score: 0.8663 - val_precision: 0.8723 - val_recall: 0.9040\n",
      "Epoch 114/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1228 - iou_score: 0.8772 - f1-score: 0.9310 - precision: 0.9394 - recall: 0.9344 - val_loss: 0.1588 - val_iou_score: 0.7964 - val_f1-score: 0.8766 - val_precision: 0.8752 - val_recall: 0.9155\n",
      "Epoch 115/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1192 - iou_score: 0.8808 - f1-score: 0.9338 - precision: 0.9404 - recall: 0.9372 - val_loss: 0.2812 - val_iou_score: 0.7790 - val_f1-score: 0.8618 - val_precision: 0.8689 - val_recall: 0.8971\n",
      "Epoch 116/200\n",
      "250/250 [==============================] - 43s 170ms/step - loss: 0.1179 - iou_score: 0.8821 - f1-score: 0.9340 - precision: 0.9412 - recall: 0.9369 - val_loss: 0.1389 - val_iou_score: 0.7910 - val_f1-score: 0.8660 - val_precision: 0.9108 - val_recall: 0.8718\n",
      "Epoch 117/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1177 - iou_score: 0.8823 - f1-score: 0.9347 - precision: 0.9417 - recall: 0.9371 - val_loss: 0.3230 - val_iou_score: 0.8044 - val_f1-score: 0.8836 - val_precision: 0.8931 - val_recall: 0.9063\n",
      "Epoch 118/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1173 - iou_score: 0.8827 - f1-score: 0.9348 - precision: 0.9425 - recall: 0.9364 - val_loss: 0.1644 - val_iou_score: 0.7872 - val_f1-score: 0.8646 - val_precision: 0.8720 - val_recall: 0.9080\n",
      "Epoch 119/200\n",
      "250/250 [==============================] - 43s 170ms/step - loss: 0.1185 - iou_score: 0.8815 - f1-score: 0.9339 - precision: 0.9414 - recall: 0.9364 - val_loss: 0.1350 - val_iou_score: 0.7812 - val_f1-score: 0.8606 - val_precision: 0.8705 - val_recall: 0.9015\n",
      "Epoch 120/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1174 - iou_score: 0.8826 - f1-score: 0.9349 - precision: 0.9416 - recall: 0.9368 - val_loss: 0.1706 - val_iou_score: 0.7927 - val_f1-score: 0.8661 - val_precision: 0.8980 - val_recall: 0.8902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1178 - iou_score: 0.8822 - f1-score: 0.9346 - precision: 0.9416 - recall: 0.9373 - val_loss: 0.1750 - val_iou_score: 0.8025 - val_f1-score: 0.8788 - val_precision: 0.9024 - val_recall: 0.8959\n",
      "Epoch 122/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1172 - iou_score: 0.8828 - f1-score: 0.9347 - precision: 0.9415 - recall: 0.9366 - val_loss: 0.2544 - val_iou_score: 0.7708 - val_f1-score: 0.8560 - val_precision: 0.8531 - val_recall: 0.9096\n",
      "Epoch 123/200\n",
      "250/250 [==============================] - 43s 170ms/step - loss: 0.1174 - iou_score: 0.8826 - f1-score: 0.9344 - precision: 0.9422 - recall: 0.9366 - val_loss: 0.4251 - val_iou_score: 0.7839 - val_f1-score: 0.8631 - val_precision: 0.8475 - val_recall: 0.9304\n",
      "Epoch 124/200\n",
      "250/250 [==============================] - 43s 170ms/step - loss: 0.1142 - iou_score: 0.8858 - f1-score: 0.9369 - precision: 0.9423 - recall: 0.9398 - val_loss: 0.3595 - val_iou_score: 0.7930 - val_f1-score: 0.8649 - val_precision: 0.8682 - val_recall: 0.9205\n",
      "Epoch 125/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1142 - iou_score: 0.8858 - f1-score: 0.9370 - precision: 0.9428 - recall: 0.9392 - val_loss: 0.1760 - val_iou_score: 0.7882 - val_f1-score: 0.8656 - val_precision: 0.8563 - val_recall: 0.9277\n",
      "Epoch 126/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1154 - iou_score: 0.8846 - f1-score: 0.9362 - precision: 0.9423 - recall: 0.9388 - val_loss: 0.1250 - val_iou_score: 0.7780 - val_f1-score: 0.8556 - val_precision: 0.8635 - val_recall: 0.9102\n",
      "Epoch 127/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1177 - iou_score: 0.8823 - f1-score: 0.9343 - precision: 0.9402 - recall: 0.9382 - val_loss: 0.3724 - val_iou_score: 0.7810 - val_f1-score: 0.8595 - val_precision: 0.8669 - val_recall: 0.9083\n",
      "Epoch 128/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1134 - iou_score: 0.8866 - f1-score: 0.9373 - precision: 0.9428 - recall: 0.9398 - val_loss: 0.1693 - val_iou_score: 0.8090 - val_f1-score: 0.8887 - val_precision: 0.8803 - val_recall: 0.9241\n",
      "Epoch 129/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1152 - iou_score: 0.8848 - f1-score: 0.9358 - precision: 0.9423 - recall: 0.9379 - val_loss: 0.1041 - val_iou_score: 0.7707 - val_f1-score: 0.8565 - val_precision: 0.8532 - val_recall: 0.8976\n",
      "Epoch 130/200\n",
      "250/250 [==============================] - 43s 170ms/step - loss: 0.1150 - iou_score: 0.8850 - f1-score: 0.9359 - precision: 0.9415 - recall: 0.9394 - val_loss: 0.2276 - val_iou_score: 0.7922 - val_f1-score: 0.8704 - val_precision: 0.8772 - val_recall: 0.9010\n",
      "Epoch 131/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1141 - iou_score: 0.8859 - f1-score: 0.9367 - precision: 0.9422 - recall: 0.9400 - val_loss: 0.1834 - val_iou_score: 0.7905 - val_f1-score: 0.8727 - val_precision: 0.8663 - val_recall: 0.9173\n",
      "Epoch 132/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1152 - iou_score: 0.8848 - f1-score: 0.9361 - precision: 0.9417 - recall: 0.9393 - val_loss: 0.1151 - val_iou_score: 0.7755 - val_f1-score: 0.8595 - val_precision: 0.8355 - val_recall: 0.9339\n",
      "Epoch 133/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1119 - iou_score: 0.8881 - f1-score: 0.9382 - precision: 0.9426 - recall: 0.9420 - val_loss: 0.2289 - val_iou_score: 0.7891 - val_f1-score: 0.8732 - val_precision: 0.8615 - val_recall: 0.9197\n",
      "Epoch 134/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1092 - iou_score: 0.8908 - f1-score: 0.9401 - precision: 0.9467 - recall: 0.9404 - val_loss: 0.1334 - val_iou_score: 0.8024 - val_f1-score: 0.8780 - val_precision: 0.8883 - val_recall: 0.9066\n",
      "Epoch 135/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1129 - iou_score: 0.8872 - f1-score: 0.9375 - precision: 0.9430 - recall: 0.9404 - val_loss: 0.1668 - val_iou_score: 0.8144 - val_f1-score: 0.8911 - val_precision: 0.9017 - val_recall: 0.9062\n",
      "Epoch 136/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1130 - iou_score: 0.8870 - f1-score: 0.9378 - precision: 0.9425 - recall: 0.9414 - val_loss: 0.1547 - val_iou_score: 0.7830 - val_f1-score: 0.8612 - val_precision: 0.8685 - val_recall: 0.9082\n",
      "Epoch 137/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1121 - iou_score: 0.8879 - f1-score: 0.9382 - precision: 0.9439 - recall: 0.9406 - val_loss: 0.2716 - val_iou_score: 0.7925 - val_f1-score: 0.8735 - val_precision: 0.8736 - val_recall: 0.9123\n",
      "Epoch 138/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1124 - iou_score: 0.8876 - f1-score: 0.9377 - precision: 0.9431 - recall: 0.9405 - val_loss: 0.3371 - val_iou_score: 0.7971 - val_f1-score: 0.8726 - val_precision: 0.8818 - val_recall: 0.9099\n",
      "Epoch 139/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1101 - iou_score: 0.8899 - f1-score: 0.9394 - precision: 0.9434 - recall: 0.9433 - val_loss: 0.1534 - val_iou_score: 0.8089 - val_f1-score: 0.8850 - val_precision: 0.8974 - val_recall: 0.9052\n",
      "Epoch 140/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1116 - iou_score: 0.8884 - f1-score: 0.9385 - precision: 0.9445 - recall: 0.9404 - val_loss: 0.2563 - val_iou_score: 0.7864 - val_f1-score: 0.8679 - val_precision: 0.8852 - val_recall: 0.8920\n",
      "Epoch 141/200\n",
      "250/250 [==============================] - 43s 170ms/step - loss: 0.1095 - iou_score: 0.8905 - f1-score: 0.9400 - precision: 0.9459 - recall: 0.9415 - val_loss: 0.1386 - val_iou_score: 0.8074 - val_f1-score: 0.8816 - val_precision: 0.9039 - val_recall: 0.8956\n",
      "Epoch 142/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1078 - iou_score: 0.8922 - f1-score: 0.9411 - precision: 0.9460 - recall: 0.9429 - val_loss: 0.2913 - val_iou_score: 0.8079 - val_f1-score: 0.8865 - val_precision: 0.8958 - val_recall: 0.9057\n",
      "Epoch 143/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1068 - iou_score: 0.8932 - f1-score: 0.9418 - precision: 0.9466 - recall: 0.9433 - val_loss: 0.1689 - val_iou_score: 0.7911 - val_f1-score: 0.8691 - val_precision: 0.8847 - val_recall: 0.8991\n",
      "Epoch 144/200\n",
      "250/250 [==============================] - 43s 170ms/step - loss: 0.1074 - iou_score: 0.8926 - f1-score: 0.9413 - precision: 0.9447 - recall: 0.9446 - val_loss: 0.2282 - val_iou_score: 0.7722 - val_f1-score: 0.8492 - val_precision: 0.8479 - val_recall: 0.9199\n",
      "Epoch 145/200\n",
      "250/250 [==============================] - 43s 170ms/step - loss: 0.1093 - iou_score: 0.8907 - f1-score: 0.9401 - precision: 0.9442 - recall: 0.9431 - val_loss: 0.1820 - val_iou_score: 0.7945 - val_f1-score: 0.8696 - val_precision: 0.8799 - val_recall: 0.9100\n",
      "Epoch 146/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1080 - iou_score: 0.8920 - f1-score: 0.9407 - precision: 0.9450 - recall: 0.9433 - val_loss: 0.2084 - val_iou_score: 0.7985 - val_f1-score: 0.8772 - val_precision: 0.8950 - val_recall: 0.8992\n",
      "Epoch 147/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1067 - iou_score: 0.8933 - f1-score: 0.9418 - precision: 0.9467 - recall: 0.9435 - val_loss: 0.2784 - val_iou_score: 0.7745 - val_f1-score: 0.8556 - val_precision: 0.8595 - val_recall: 0.9062\n",
      "Epoch 148/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1064 - iou_score: 0.8936 - f1-score: 0.9420 - precision: 0.9459 - recall: 0.9444 - val_loss: 0.2192 - val_iou_score: 0.8000 - val_f1-score: 0.8720 - val_precision: 0.8835 - val_recall: 0.9078\n",
      "Epoch 149/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1050 - iou_score: 0.8950 - f1-score: 0.9428 - precision: 0.9483 - recall: 0.9438 - val_loss: 0.3679 - val_iou_score: 0.8004 - val_f1-score: 0.8772 - val_precision: 0.8886 - val_recall: 0.9067\n",
      "Epoch 150/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1047 - iou_score: 0.8953 - f1-score: 0.9430 - precision: 0.9484 - recall: 0.9438 - val_loss: 0.1665 - val_iou_score: 0.7960 - val_f1-score: 0.8727 - val_precision: 0.8734 - val_recall: 0.9156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1047 - iou_score: 0.8953 - f1-score: 0.9430 - precision: 0.9467 - recall: 0.9453 - val_loss: 0.1761 - val_iou_score: 0.7808 - val_f1-score: 0.8591 - val_precision: 0.8648 - val_recall: 0.9110\n",
      "Epoch 152/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1055 - iou_score: 0.8945 - f1-score: 0.9422 - precision: 0.9460 - recall: 0.9454 - val_loss: 0.3769 - val_iou_score: 0.7859 - val_f1-score: 0.8627 - val_precision: 0.8765 - val_recall: 0.9032\n",
      "Epoch 153/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1050 - iou_score: 0.8950 - f1-score: 0.9428 - precision: 0.9460 - recall: 0.9459 - val_loss: 0.1503 - val_iou_score: 0.8219 - val_f1-score: 0.8964 - val_precision: 0.9033 - val_recall: 0.9135\n",
      "Epoch 154/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1044 - iou_score: 0.8956 - f1-score: 0.9432 - precision: 0.9470 - recall: 0.9455 - val_loss: 0.0992 - val_iou_score: 0.7789 - val_f1-score: 0.8586 - val_precision: 0.8560 - val_recall: 0.9169\n",
      "Epoch 155/200\n",
      "250/250 [==============================] - 43s 170ms/step - loss: 0.1023 - iou_score: 0.8977 - f1-score: 0.9445 - precision: 0.9488 - recall: 0.9459 - val_loss: 0.1954 - val_iou_score: 0.7934 - val_f1-score: 0.8704 - val_precision: 0.8697 - val_recall: 0.9174\n",
      "Epoch 156/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1038 - iou_score: 0.8962 - f1-score: 0.9432 - precision: 0.9485 - recall: 0.9441 - val_loss: 0.1737 - val_iou_score: 0.8035 - val_f1-score: 0.8797 - val_precision: 0.8896 - val_recall: 0.9094\n",
      "Epoch 157/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1039 - iou_score: 0.8961 - f1-score: 0.9433 - precision: 0.9470 - recall: 0.9455 - val_loss: 0.1210 - val_iou_score: 0.7977 - val_f1-score: 0.8742 - val_precision: 0.8674 - val_recall: 0.9253\n",
      "Epoch 158/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1033 - iou_score: 0.8967 - f1-score: 0.9436 - precision: 0.9469 - recall: 0.9462 - val_loss: 0.2178 - val_iou_score: 0.7822 - val_f1-score: 0.8619 - val_precision: 0.8593 - val_recall: 0.9163\n",
      "Epoch 159/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1018 - iou_score: 0.8982 - f1-score: 0.9449 - precision: 0.9491 - recall: 0.9460 - val_loss: 0.1441 - val_iou_score: 0.7891 - val_f1-score: 0.8640 - val_precision: 0.8838 - val_recall: 0.9000\n",
      "Epoch 160/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1015 - iou_score: 0.8985 - f1-score: 0.9448 - precision: 0.9483 - recall: 0.9470 - val_loss: 0.1626 - val_iou_score: 0.8192 - val_f1-score: 0.8946 - val_precision: 0.9071 - val_recall: 0.9075\n",
      "Epoch 161/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1030 - iou_score: 0.8970 - f1-score: 0.9441 - precision: 0.9474 - recall: 0.9464 - val_loss: 0.1729 - val_iou_score: 0.7786 - val_f1-score: 0.8576 - val_precision: 0.8533 - val_recall: 0.9186\n",
      "Epoch 162/200\n",
      "250/250 [==============================] - 43s 170ms/step - loss: 0.1022 - iou_score: 0.8978 - f1-score: 0.9442 - precision: 0.9487 - recall: 0.9455 - val_loss: 0.2630 - val_iou_score: 0.7887 - val_f1-score: 0.8657 - val_precision: 0.8836 - val_recall: 0.8983\n",
      "Epoch 163/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1017 - iou_score: 0.8983 - f1-score: 0.9448 - precision: 0.9486 - recall: 0.9466 - val_loss: 0.3195 - val_iou_score: 0.7995 - val_f1-score: 0.8738 - val_precision: 0.8854 - val_recall: 0.9093\n",
      "Epoch 164/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1027 - iou_score: 0.8973 - f1-score: 0.9439 - precision: 0.9482 - recall: 0.9455 - val_loss: 0.1683 - val_iou_score: 0.8072 - val_f1-score: 0.8830 - val_precision: 0.8786 - val_recall: 0.9240\n",
      "Epoch 165/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1010 - iou_score: 0.8990 - f1-score: 0.9452 - precision: 0.9493 - recall: 0.9462 - val_loss: 0.2829 - val_iou_score: 0.7781 - val_f1-score: 0.8579 - val_precision: 0.8629 - val_recall: 0.9094\n",
      "Epoch 166/200\n",
      "250/250 [==============================] - 43s 170ms/step - loss: 0.0998 - iou_score: 0.9002 - f1-score: 0.9461 - precision: 0.9495 - recall: 0.9478 - val_loss: 0.1375 - val_iou_score: 0.7947 - val_f1-score: 0.8676 - val_precision: 0.8842 - val_recall: 0.9061\n",
      "Epoch 167/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1017 - iou_score: 0.8983 - f1-score: 0.9450 - precision: 0.9491 - recall: 0.9461 - val_loss: 0.2940 - val_iou_score: 0.8111 - val_f1-score: 0.8885 - val_precision: 0.9006 - val_recall: 0.9059\n",
      "Epoch 168/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1016 - iou_score: 0.8984 - f1-score: 0.9445 - precision: 0.9484 - recall: 0.9465 - val_loss: 0.1457 - val_iou_score: 0.7844 - val_f1-score: 0.8616 - val_precision: 0.8583 - val_recall: 0.9215\n",
      "Epoch 169/200\n",
      "250/250 [==============================] - 43s 170ms/step - loss: 0.1005 - iou_score: 0.8995 - f1-score: 0.9455 - precision: 0.9487 - recall: 0.9477 - val_loss: 0.1280 - val_iou_score: 0.7894 - val_f1-score: 0.8663 - val_precision: 0.8759 - val_recall: 0.9073\n",
      "Epoch 170/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1007 - iou_score: 0.8993 - f1-score: 0.9453 - precision: 0.9482 - recall: 0.9476 - val_loss: 0.1858 - val_iou_score: 0.7952 - val_f1-score: 0.8707 - val_precision: 0.8759 - val_recall: 0.9148\n",
      "Epoch 171/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1008 - iou_score: 0.8992 - f1-score: 0.9454 - precision: 0.9495 - recall: 0.9467 - val_loss: 0.2043 - val_iou_score: 0.8092 - val_f1-score: 0.8849 - val_precision: 0.8943 - val_recall: 0.9102\n",
      "Epoch 172/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1003 - iou_score: 0.8997 - f1-score: 0.9458 - precision: 0.9488 - recall: 0.9479 - val_loss: 0.2565 - val_iou_score: 0.7781 - val_f1-score: 0.8573 - val_precision: 0.8651 - val_recall: 0.9071\n",
      "Epoch 173/200\n",
      "250/250 [==============================] - 43s 170ms/step - loss: 0.1009 - iou_score: 0.8991 - f1-score: 0.9452 - precision: 0.9491 - recall: 0.9468 - val_loss: 0.2149 - val_iou_score: 0.7997 - val_f1-score: 0.8731 - val_precision: 0.8814 - val_recall: 0.9116\n",
      "Epoch 174/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.0997 - iou_score: 0.9003 - f1-score: 0.9457 - precision: 0.9491 - recall: 0.9478 - val_loss: 0.3688 - val_iou_score: 0.7996 - val_f1-score: 0.8769 - val_precision: 0.8855 - val_recall: 0.9089\n",
      "Epoch 175/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1000 - iou_score: 0.9000 - f1-score: 0.9459 - precision: 0.9501 - recall: 0.9469 - val_loss: 0.1620 - val_iou_score: 0.7990 - val_f1-score: 0.8765 - val_precision: 0.8701 - val_recall: 0.9235\n",
      "Epoch 176/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1007 - iou_score: 0.8993 - f1-score: 0.9453 - precision: 0.9487 - recall: 0.9476 - val_loss: 0.1732 - val_iou_score: 0.7910 - val_f1-score: 0.8697 - val_precision: 0.8708 - val_recall: 0.9144\n",
      "Epoch 177/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.0998 - iou_score: 0.9002 - f1-score: 0.9458 - precision: 0.9500 - recall: 0.9468 - val_loss: 0.3744 - val_iou_score: 0.7824 - val_f1-score: 0.8604 - val_precision: 0.8660 - val_recall: 0.9113\n",
      "Epoch 178/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1005 - iou_score: 0.8995 - f1-score: 0.9457 - precision: 0.9476 - recall: 0.9488 - val_loss: 0.1439 - val_iou_score: 0.8192 - val_f1-score: 0.8947 - val_precision: 0.9012 - val_recall: 0.9140\n",
      "Epoch 179/200\n",
      "250/250 [==============================] - 42s 168ms/step - loss: 0.1003 - iou_score: 0.8997 - f1-score: 0.9455 - precision: 0.9496 - recall: 0.9467 - val_loss: 0.0935 - val_iou_score: 0.7821 - val_f1-score: 0.8623 - val_precision: 0.8610 - val_recall: 0.9145\n",
      "Epoch 180/200\n",
      "\n",
      "Epoch 00180: starting stochastic weight averaging\n",
      "250/250 [==============================] - 43s 170ms/step - loss: 0.1009 - iou_score: 0.8991 - f1-score: 0.9451 - precision: 0.9487 - recall: 0.9473 - val_loss: 0.1759 - val_iou_score: 0.7978 - val_f1-score: 0.8740 - val_precision: 0.8853 - val_recall: 0.9059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1001 - iou_score: 0.8999 - f1-score: 0.9457 - precision: 0.9491 - recall: 0.9479 - val_loss: 0.1692 - val_iou_score: 0.8003 - val_f1-score: 0.8775 - val_precision: 0.8870 - val_recall: 0.9079\n",
      "Epoch 182/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1002 - iou_score: 0.8998 - f1-score: 0.9456 - precision: 0.9480 - recall: 0.9486 - val_loss: 0.1306 - val_iou_score: 0.8061 - val_f1-score: 0.8820 - val_precision: 0.8808 - val_recall: 0.9200\n",
      "Epoch 183/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.1016 - iou_score: 0.8984 - f1-score: 0.9446 - precision: 0.9475 - recall: 0.9473 - val_loss: 0.2083 - val_iou_score: 0.7917 - val_f1-score: 0.8705 - val_precision: 0.8757 - val_recall: 0.9094\n",
      "Epoch 184/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1006 - iou_score: 0.8994 - f1-score: 0.9452 - precision: 0.9481 - recall: 0.9482 - val_loss: 0.1446 - val_iou_score: 0.7937 - val_f1-score: 0.8693 - val_precision: 0.8885 - val_recall: 0.8993\n",
      "Epoch 185/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.0998 - iou_score: 0.9002 - f1-score: 0.9457 - precision: 0.9495 - recall: 0.9473 - val_loss: 0.1581 - val_iou_score: 0.8176 - val_f1-score: 0.8935 - val_precision: 0.9022 - val_recall: 0.9104\n",
      "Epoch 186/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.0987 - iou_score: 0.9013 - f1-score: 0.9467 - precision: 0.9506 - recall: 0.9476 - val_loss: 0.1727 - val_iou_score: 0.7803 - val_f1-score: 0.8609 - val_precision: 0.8529 - val_recall: 0.9207\n",
      "Epoch 187/200\n",
      "250/250 [==============================] - 43s 170ms/step - loss: 0.0991 - iou_score: 0.9009 - f1-score: 0.9462 - precision: 0.9500 - recall: 0.9474 - val_loss: 0.2675 - val_iou_score: 0.7841 - val_f1-score: 0.8633 - val_precision: 0.8707 - val_recall: 0.9065\n",
      "Epoch 188/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.1000 - iou_score: 0.9000 - f1-score: 0.9458 - precision: 0.9489 - recall: 0.9478 - val_loss: 0.3429 - val_iou_score: 0.7964 - val_f1-score: 0.8718 - val_precision: 0.8791 - val_recall: 0.9123\n",
      "Epoch 189/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.0997 - iou_score: 0.9003 - f1-score: 0.9458 - precision: 0.9492 - recall: 0.9477 - val_loss: 0.1702 - val_iou_score: 0.8078 - val_f1-score: 0.8847 - val_precision: 0.8814 - val_recall: 0.9212\n",
      "Epoch 190/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.0976 - iou_score: 0.9024 - f1-score: 0.9472 - precision: 0.9510 - recall: 0.9480 - val_loss: 0.2947 - val_iou_score: 0.7712 - val_f1-score: 0.8534 - val_precision: 0.8539 - val_recall: 0.9124\n",
      "Epoch 191/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.0987 - iou_score: 0.9013 - f1-score: 0.9467 - precision: 0.9495 - recall: 0.9488 - val_loss: 0.1356 - val_iou_score: 0.7958 - val_f1-score: 0.8694 - val_precision: 0.8835 - val_recall: 0.9077\n",
      "Epoch 192/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.0995 - iou_score: 0.9005 - f1-score: 0.9459 - precision: 0.9485 - recall: 0.9483 - val_loss: 0.3124 - val_iou_score: 0.8077 - val_f1-score: 0.8860 - val_precision: 0.8912 - val_recall: 0.9111\n",
      "Epoch 193/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.0982 - iou_score: 0.9018 - f1-score: 0.9467 - precision: 0.9499 - recall: 0.9486 - val_loss: 0.1739 - val_iou_score: 0.7900 - val_f1-score: 0.8675 - val_precision: 0.8753 - val_recall: 0.9092\n",
      "Epoch 194/200\n",
      "250/250 [==============================] - 43s 170ms/step - loss: 0.0984 - iou_score: 0.9016 - f1-score: 0.9469 - precision: 0.9510 - recall: 0.9475 - val_loss: 0.1176 - val_iou_score: 0.7929 - val_f1-score: 0.8705 - val_precision: 0.8829 - val_recall: 0.9026\n",
      "Epoch 195/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.0975 - iou_score: 0.9025 - f1-score: 0.9473 - precision: 0.9509 - recall: 0.9488 - val_loss: 0.1895 - val_iou_score: 0.7939 - val_f1-score: 0.8700 - val_precision: 0.8822 - val_recall: 0.9061\n",
      "Epoch 196/200\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.0975 - iou_score: 0.9025 - f1-score: 0.9475 - precision: 0.9510 - recall: 0.9485 - val_loss: 0.2113 - val_iou_score: 0.8066 - val_f1-score: 0.8857 - val_precision: 0.8908 - val_recall: 0.9103\n",
      "Epoch 197/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.0980 - iou_score: 0.9020 - f1-score: 0.9471 - precision: 0.9501 - recall: 0.9488 - val_loss: 0.2586 - val_iou_score: 0.7717 - val_f1-score: 0.8552 - val_precision: 0.8548 - val_recall: 0.9104\n",
      "Epoch 198/200\n",
      "250/250 [==============================] - 43s 170ms/step - loss: 0.0976 - iou_score: 0.9024 - f1-score: 0.9475 - precision: 0.9506 - recall: 0.9489 - val_loss: 0.2340 - val_iou_score: 0.7961 - val_f1-score: 0.8718 - val_precision: 0.8752 - val_recall: 0.9146\n",
      "Epoch 199/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.0984 - iou_score: 0.9016 - f1-score: 0.9466 - precision: 0.9508 - recall: 0.9473 - val_loss: 0.3821 - val_iou_score: 0.7937 - val_f1-score: 0.8732 - val_precision: 0.8797 - val_recall: 0.9083\n",
      "Epoch 200/200\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.0974 - iou_score: 0.9026 - f1-score: 0.9473 - precision: 0.9498 - recall: 0.9498 - val_loss: 0.1724 - val_iou_score: 0.7923 - val_f1-score: 0.8723 - val_precision: 0.8632 - val_recall: 0.9235\n",
      "\n",
      "Epoch 00201: final model weights set to stochastic weight average\n",
      "Success Saving Plot\n",
      "Success Saving Model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_fit(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "ieEvXtlAv900",
    "outputId": "6fcd4d37-75bb-432e-8b43-06a7a7efa626"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for MBV3_UNet_nobottom-2_aug_swaConst20_jacc_0.001_200_192x256_yes\n",
      "Loss: 0.12883\n",
      "mean iou_score: 0.79264\n",
      "mean f1-score: 0.8694\n",
      "mean precision: 0.91514\n",
      "mean recall: 0.87044\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(save_dir[0]+model.name+\"/\"+model.name+\"_last_weight.h5\")\n",
    "\n",
    "test_eval(model, 'last')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "NgRrO4WKwABw",
    "outputId": "3ac4302c-f88d-481b-eb56-55d2c9278b49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for MBV3_UNet_nobottom-2_aug_swaConst20_jacc_0.001_200_192x256_yes\n",
      "Loss: 0.10915\n",
      "mean iou_score: 0.79385\n",
      "mean f1-score: 0.87039\n",
      "mean precision: 0.91353\n",
      "mean recall: 0.87311\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(save_dir[0]+model.name+\"/\"+model.name+\"_best_weights.h5\")\n",
    "\n",
    "test_eval(model, 'best')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8uH0atUjuUHP"
   },
   "source": [
    "#### Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aKeUbl0ekmNR"
   },
   "outputs": [],
   "source": [
    "def HFITH(output_mask):\n",
    "  try:\n",
    "    contours1, hierarchy1 = cv2.findContours(np.array(output_mask, dtype=np.uint8).squeeze(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) \n",
    "    largest_contour1 = []\n",
    "    largest_area = 0\n",
    "    for contour in contours1:\n",
    "        area = cv2.contourArea(contour)\n",
    "        if area > largest_area:\n",
    "            largest_area = area\n",
    "            largest_contour1 = contour\n",
    "    img = np.zeros([output_mask.shape[1],output_mask.shape[2],3])\n",
    "    img = cv2.drawContours(img, [largest_contour1], -1, (0,255,0), -1)\n",
    "    img = cv2.threshold(img, 254, 255, cv2.THRESH_BINARY)[1]\n",
    "    img = cv2.cvtColor(np.uint8(img), cv2.COLOR_BGR2GRAY) \n",
    "    return np.expand_dims(img, axis=2)\n",
    "  except:\n",
    "    return output_mask\n",
    "\n",
    "def FITH(output_mask):\n",
    "  try:\n",
    "    contours1, hierarchy1 = cv2.findContours(np.array(output_mask, dtype=np.uint8).squeeze(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) \n",
    "    largest_contour1 = []\n",
    "    largest_area = 0\n",
    "    for contour in contours1:\n",
    "        area = cv2.contourArea(contour)\n",
    "        if area > largest_area:\n",
    "            largest_area = area\n",
    "            largest_contour1 = contour\n",
    "    img = np.zeros([output_mask.shape[1],output_mask.shape[2],3])\n",
    "    img = cv2.drawContours(img, [largest_contour1], -1, (0,255,0), -1)\n",
    "    img = cv2.threshold(img, 254, 255, cv2.THRESH_BINARY)[1]\n",
    "    img = cv2.cvtColor(np.uint8(img), cv2.COLOR_BGR2GRAY) \n",
    "    img = np.expand_dims(img, axis=2)\n",
    "    return np.where(img>output_mask,img,output_mask)\n",
    "  except:\n",
    "    return output_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test\n",
    "fill_preds = np.zeros([600,192,256,1])\n",
    "fill_preds2 = np.zeros([600,192,256,1])\n",
    "gt_masks = np.zeros([600,192,256,1])\n",
    "pr_masks = np.zeros([600,192,256,1])\n",
    "\n",
    "## Validation\n",
    "fill_preds_val = np.zeros([150,192,256,1])\n",
    "fill_preds2_val = np.zeros([150,192,256,1])\n",
    "gt_masks_val = np.zeros([150,192,256,1])\n",
    "pr_masks_val = np.zeros([150,192,256,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MBNetV3_BCDU_normal_swaConst20_jacc_0.001_200_192x256_yes\n",
      "\n",
      "Jaccard index (best): 0.7177036\n",
      "Jaccard index with FITH (best): 0.71790195\n",
      "Jaccard index with H-FITH (best): 0.7164134\n",
      "\n",
      "Jaccard index (last): 0.7120504\n",
      "Jaccard index with FITH (last): 0.7122888\n",
      "Jaccard index with H-FITH (last): 0.71245724\n"
     ]
    }
   ],
   "source": [
    "i_ = 1\n",
    "\n",
    "model.load_weights(save_dir[0]+model.name+\"/\"+model.name+\"_best_weights.h5\")\n",
    "for i in range(600):\n",
    "  i_+=1\n",
    "  image, gt_masks[i,:,:,:] = test_gen[i]\n",
    "  image = np.expand_dims(image[0], axis=0)\n",
    "  # 0 ... 1\n",
    "  output_mask = model.predict(image)\n",
    "  pr_masks[i,:,:,:] = output_mask\n",
    "  # 0 ... 255 -> 0 ... 1\n",
    "  fill_preds[i,:,:,:] = FITH(output_mask*255)/255\n",
    "  fill_preds2[i,:,:,:] = HFITH(output_mask*255)/255\n",
    "\n",
    "    \n",
    "#Jaccard index\n",
    "print(model.name)\n",
    "jai = sm.metrics.IOUScore(per_image=True, threshold=0.5)(gt_masks, pr_masks)\n",
    "print (\"\\nJaccard index (best): \" +str(jai.numpy()))\n",
    "jai = sm.metrics.IOUScore(per_image=True, threshold=0.5)(gt_masks, fill_preds)\n",
    "print (\"Jaccard index with FITH (best): \" +str(jai.numpy()))\n",
    "jai = sm.metrics.IOUScore(per_image=True, threshold=0.5)(gt_masks, fill_preds2)\n",
    "print (\"Jaccard index with H-FITH (best): \" +str(jai.numpy()))\n",
    "\n",
    "i_ = 1\n",
    "\n",
    "model.load_weights(save_dir[0]+model.name+\"/\"+model.name+\"_last_weight.h5\")\n",
    "for i in range(600):\n",
    "  i_+=1\n",
    "  image, gt_masks[i,:,:,:] = test_gen[i]\n",
    "  image = np.expand_dims(image[0], axis=0)\n",
    "  # 0 ... 1\n",
    "  output_mask = model.predict(image)\n",
    "  pr_masks[i,:,:,:] = output_mask\n",
    "  # 0 ... 255 -> 0 ... 1\n",
    "  fill_preds[i,:,:,:] = FITH(output_mask*255)/255\n",
    "  fill_preds2[i,:,:,:] = HFITH(output_mask*255)/255\n",
    "\n",
    "    \n",
    "#Jaccard index\n",
    "jai = sm.metrics.IOUScore(per_image=True, threshold=0.5)(gt_masks, pr_masks)\n",
    "print (\"\\nJaccard index (last): \" +str(jai.numpy()))\n",
    "jai = sm.metrics.IOUScore(per_image=True, threshold=0.5)(gt_masks, fill_preds)\n",
    "print (\"Jaccard index with FITH (last): \" +str(jai.numpy()))\n",
    "jai = sm.metrics.IOUScore(per_image=True, threshold=0.5)(gt_masks, fill_preds2)\n",
    "print (\"Jaccard index with H-FITH (last): \" +str(jai.numpy()))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "yzVZm83Lt_oI",
    "rNMCEm1WuH1b",
    "ALukzNVmo6JH",
    "YRHfMJde8W-L"
   ],
   "name": "2_Re_run_MobileNetv3_Unet_(27_26_Juli).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
